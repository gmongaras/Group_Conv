{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c966fe0-b91e-4bcd-bf39-1a473e8a0b44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7c4959-777f-4ac9-b8dc-3b8401481295",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a random image of shape (1, C, N, W)\n",
    "inCh = 10\n",
    "outCh = 13\n",
    "L = 12\n",
    "W = 13\n",
    "image = torch.rand(1, inCh, L, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8292447e-2185-4782-9181-4ca767372d64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Random convolution weight of shape (outCh, inCh, kernel_height, kernel_width)\n",
    "kernel_height = 3\n",
    "kernel_width = 4\n",
    "conv_weight = torch.rand(outCh, inCh, kernel_height, kernel_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca62625a-ab7e-4a98-974f-603582f046b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functional convolution to get the real output for checking\n",
    "conv_out = torch.nn.functional.conv2d(image, conv_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbf2476-ec0d-4894-87c4-0176e8cc2f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a36dae-5954-4f11-adde-189377bd1a86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2d9aa04-b787-489b-add0-27ea9838c4b8",
   "metadata": {},
   "source": [
    "# Manual convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "431c7ba5-b13c-4610-8c35-f560525d2391",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 12, 13])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe091925-51a4-4e64-a0fc-a1d764016362",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Unfold image\n",
    "patches = image.unfold(2, kernel_height, 1).unfold(3, kernel_width, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fafa37f-c411-4f52-903e-e14f82a2712a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 10, 10, 3, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size, channels, h_windows, w_windows, kernel_height, kernel_width\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96bea85f-5f43-42e8-8bb2-b1dc345c574e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 100, 3, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches = patches.contiguous().view(patches.shape[0], patches.shape[1], -1, kernel_height, kernel_width)\n",
    "# batch_size, channels, windows, kernel_height, kernel_width\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11cab0d3-cb3f-48ff-bf10-aec2916e84d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 10, 3, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shift the windows into the batch dimension using permute\n",
    "patches = patches.permute(0, 2, 1, 3, 4)\n",
    "# batch_size, windows, channels, kernel_height, kernel_width\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fea9fa56-2e9e-49d1-b98a-2663844eb3cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 10, 3, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0deb3583-5562-4741-8ca3-571ceddfc0aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Multiply the patches with the weights in order to calculate the conv\n",
    "result = (patches.unsqueeze(2) * conv_weight.unsqueeze(0).unsqueeze(1)).sum([3, 4, 5])\n",
    "result = result.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03ebae5e-0ae9-4214-929e-065a9e3749e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 100])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d840be05-46bb-47aa-b1ea-4c73481bfa77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reshape to height and width\n",
    "h = w = int(result.size(2)**0.5)\n",
    "result = result.view(result.shape[0], -1, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "261a7d33-761c-4f3f-b6c5-252e84c251ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(result.round(decimals=2) == conv_out.round(decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "050e2c6b-d029-4f47-967e-7dbe4e1cead1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0048)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = (((result-conv_out)**2)**0.5).sum()\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efad3efc-2cef-4de7-940f-a636fdfd9d45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdbdbce-d06a-409b-a893-dfc0e7dddf6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cc1c57-c5c7-4378-ac24-5dd2baf83712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd713585-06f9-46a5-97be-3338d2bd6413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3bc58f-73db-4f69-b88c-622b8c4f2a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1c0ce6-2684-4c64-85bf-d3a40de19be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1e7a857-923b-416e-9e7d-0bdf626c4279",
   "metadata": {
    "tags": []
   },
   "source": [
    "### What I want is a tensor that's unrolled in two ways to be of shape (N, C//sub_size, sub_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96061551-3f0d-4f18-a87d-6a4dabb686f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f3bbb49-1564-4ebf-8b3a-7f5913ff4f56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 12, 13])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4657a2ae-1e75-4eff-aa4e-f5bd8a0e06a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I think ContinuousPad1d will work.\n",
    "# If I have a sequence 1,2,3,4,5,6 and I pad by 2, I want 1,2,3,4,5,6,1,2\n",
    "samp = torch.tensor([1,2,3,4,5,6], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11a132a4-4329-4973-b90a-b034e1cf6d6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2., 3., 4., 5., 6., 1., 2.]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This looks like it works\n",
    "torch.nn.functional.pad(input=samp.unsqueeze(0).unsqueeze(0), pad=(0,2), mode=\"circular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47c9f194-e4a9-49c8-ba46-a2d5e871c3e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 12, 13])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b00f54ae-308d-4e21-8353-caa0a098f848",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I need to pad the image by sub_size-1 along the channels to become (N, C+sub_size-1, L, W)\n",
    "image_pad = torch.nn.functional.pad(input=image.unsqueeze(0), pad=(0,0,0,0,0,sub_size-1), mode=\"circular\").squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6767d1c-145b-47d7-b88e-57f6b4fc0fd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 12, 13])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size, outCh, L, W\n",
    "image_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f887fc38-e0d8-49d8-b6fd-d69355c10fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's make sure the last `sub_size-1` channels are equal to the first `sub_size-1` channels\n",
    "assert torch.all(image_pad[:, :sub_size-1] == image_pad[:, image_pad.shape[1]-sub_size+1:])\n",
    "# and the the first channels are equal to the first channels in the original image\n",
    "assert torch.all(image_pad[:, :sub_size-1] == image[:, :sub_size-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9969245f-df46-41bc-aaf5-d75f8a4729d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6007a9c0-1591-479d-97f0-6785857fc0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f188ef-f3b0-4954-b964-40c5c8a64c33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2e70b6d3-02e6-492c-ba7e-5cf90daa1768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's find out what the weight should look like.\n",
    "# We want outCh number of kernels which take in sub_size and output 1\n",
    "# which will be stacked to get a shape of outCh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9051305e-b914-4528-b82f-a91e95e32834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convolutions\n",
    "convs = [nn.Conv2d(sub_size, 1, (kernel_height, kernel_width)) for i in range(0, outCh)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "28d6eda1-0585-4016-a9c7-f9918e27673b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's get all the weights\n",
    "weight = torch.stack([i.weight for i in convs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0801e99f-80f9-46cb-9bdc-dd2f1a615484",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 4, 3, 4])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's the shape?\n",
    "weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0c56085b-cb5c-4ea0-973c-e69b8674eead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_conv_weight = weight.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4fbc03d7-434c-4ab2-ba2f-d749752e2825",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Shape is (outCh, 1, sub_size, kernel_height, kernel_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "34230e96-5c67-478d-969a-c6bbb3f675df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old: 1560\n",
      "New: 624\n",
      "New/Old: 2.5\n"
     ]
    }
   ],
   "source": [
    "# This total size should be a lot less than the original\n",
    "print(f\"Old: {torch.prod(torch.tensor(conv_weight.shape)).item()}\")\n",
    "print(f\"New: {torch.prod(torch.tensor(weight.shape)).item()}\")\n",
    "print(f\"New/Old: {torch.prod(torch.tensor(conv_weight.shape)).item()/torch.prod(torch.tensor(weight.shape)).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "24daa846-aba8-48f6-915d-1bd7178e3ddc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outCh/Sub size = 2.5\n"
     ]
    }
   ],
   "source": [
    "# This is the expected value as that's the number of times sub_size goes into inCh\n",
    "print(f\"outCh/Sub size = {inCh/sub_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dd1519-e692-4138-acb9-eceffbad8a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa905f6-10af-4649-8059-48923293fe34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ec8702-8207-49f0-8701-61a592ceb356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db558135-d6cc-4c75-b49d-b54f383ea1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9fc967-f86e-4d66-8c5a-9e1924775b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "acf6b719-9ff0-4578-8f39-49a8991b58e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, we need to pad the original image to have (outCh-sub_size-1) channels (N, outC+sub_size-1, L, W)\n",
    "# When it unrolls, there will be outCh number of images with sub_size channels each\n",
    "\n",
    "# Number of desired channels\n",
    "desired_channels = outCh+sub_size-1\n",
    "# Number of times to repeat the tensor to get to that goal\n",
    "num_repeats = math.ceil(desired_channels/inCh)\n",
    "# Repeat the image num_repeats times along the channels\n",
    "image_pad = image.repeat(1, num_repeats, 1, 1)\n",
    "# Slice the rest off that we don't need\n",
    "image_pad = image_pad[:, :desired_channels]\n",
    "# image_pad = torch.nn.functional.pad(input=image.unsqueeze(0), pad=(0,0,0,0,0,outCh+sub_size-1), mode=\"circular\").squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ffbd42e6-58b3-4286-9cd9-c475af3a4672",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 12, 13])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "df1baea9-92ba-4cba-84a5-8aab2e98b990",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 10, 10, 3, 4])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unfold image\n",
    "patches = image_pad.unfold(2, kernel_height, 1).unfold(3, kernel_width, 1)\n",
    "# batch_size, channels+sub_size-1, h_windows, w_windows, kernel_height, kernel_width\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "97f33b16-d434-4891-9021-d09c71ba453c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 100, 3, 4])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches = patches.contiguous().view(patches.shape[0], patches.shape[1], -1, kernel_height, kernel_width)\n",
    "# batch_size, channels+sub_size-1, windows, kernel_height, kernel_width\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b2573dce-f3bc-4ca5-9de8-20dccf4c2b69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 4, 100, 3, 4])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's unfold this tensor to be of shape (batch_size, windows, outCh, sub_size, kernel_height, kernel_width)\n",
    "patches = patches.unfold(1, sub_size, 1)\n",
    "# We now have a tensor of shape (batch_size, outCh, kernel_height, kernel_width, sub_size)\n",
    "patches = patches.view(patches.shape[0], patches.shape[1], sub_size, -1, kernel_height, kernel_width)\n",
    "# Now we have one of shape (batch_size, outCh, sub_size, windows, kernel_height, kernel_width)\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c26fec47-15dc-4639-b13c-a7a3146c8daf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 13, 4, 3, 4])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shift the windows into the batch dimension using permute\n",
    "patches = patches.permute(0, 3, 1, 2, 4, 5)\n",
    "# batch_size, windows, outCh, sub_size, kernel_height, kernel_width\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "49ad2f42-1bd4-4057-9d04-93036da39458",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 4, 3, 4])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our weight shape (outCh, 1, sub_size, kernel_height, kernel_width)\n",
    "new_conv_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4b28a353-95f2-43bc-85be-eb760c57b041",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 13, 13, 4, 3, 4])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(patches.unsqueeze(2) * new_conv_weight.unsqueeze(0).unsqueeze(1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7328c077-564f-444a-bc90-1c6e73fdcf39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 13, 4, 3, 4])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches.shape  # batch_size, windows, inCh, sub_size, kernel_height, kernel_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f425ed32-b9d3-41fa-b0e6-2382ef5264b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 4, 3, 4])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_conv_weight.shape # outCh, 1, sub_size, kernel_height, kernel_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d7f0cb82-05c3-4868-943d-2e9338fa73d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 13, 4, 3, 4])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "848c1996-2ed7-42fc-8332-fd4b12572a92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 13, 4, 3, 4])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_conv_weight.transpose(0, 1).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4a326-e98f-4f87-94eb-7d0ed43edf23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "496a2d93-e634-422c-8fb6-d89aa462b312",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 100])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply the patches with the weights in order to calculate the conv\n",
    "result = (patches * new_conv_weight.transpose(0, 1).unsqueeze(0)).sum([3, 4, 5])\n",
    "result = result.permute(0, 2, 1)\n",
    "# batch_size, outCh, HW\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fd632c09-ef1c-4999-b9ca-eabacebdb5ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 10, 10])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape to height and width\n",
    "h = w = int(result.size(2)**0.5)\n",
    "result = result.view(result.shape[0], -1, h, w)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9d38fb-7d6b-4d6e-bac5-f624001813b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47cd16e-a0b9-4c43-b435-bfea0e084fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb50a23-630b-452d-b780-9a9c0a78f971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd2596-c440-4ca7-ac9d-8e9b163a24c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c7f9336-2559-47e0-8a40-03a92f04b37c",
   "metadata": {},
   "source": [
    "# Custom convolution class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c1b0317b-df1d-4d57-94fb-58a8cad25bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now let's try this with multiple output channels and a\n",
    "# subset of inputs channels\n",
    "class Sparse_Conv_old(nn.Module):\n",
    "    def __init__(self, inCh, outCh, kernel, sub):\n",
    "        super(Sparse_Conv_old, self).__init__()\n",
    "        \n",
    "        assert sub <= inCh\n",
    "        self.sub = sub\n",
    "        self.inCh = inCh\n",
    "        self.outCh = outCh\n",
    "        \n",
    "        # self.convs = nn.ParameterList([nn.Conv2d(sub, 1, kernel) for i in range(0, outCh)])\n",
    "        \n",
    "    def forward(self, X):\n",
    "        if len(X.shape) == 3:\n",
    "            X = X.unsqueeze(0)\n",
    "        \n",
    "        # Output tensor\n",
    "        out = []\n",
    "        \n",
    "        # Iterate over all convolutions\n",
    "        for i, c in enumerate(self.convs):\n",
    "            # Current subset\n",
    "            lower = math.floor((self.inCh/self.outCh)*i)\n",
    "            upper = min(self.inCh, lower+self.sub)\n",
    "            extra = max(0, lower+self.sub-self.inCh)\n",
    "            print(lower, upper, extra)\n",
    "            \n",
    "            # Convolution\n",
    "            if extra == 0:\n",
    "                sub = X[:, lower:upper]\n",
    "            else:\n",
    "                sub = torch.cat((X[:, lower:upper], X[:, 0:extra]), dim=1)\n",
    "            out.append(c(sub).squeeze(1))\n",
    "        \n",
    "        # Stack the output and return it\n",
    "        return torch.stack(out).permute(1, 0, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c416bf8b-3db5-48c6-bb8d-da410bf23f5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now let's try this with multiple output channels and a\n",
    "# subset of inputs channels\n",
    "class Sparse_Conv_new(nn.Module):\n",
    "    def __init__(self, inCh, outCh, kernel_size, sub_size):\n",
    "        super(Sparse_Conv_new, self).__init__()\n",
    "        \n",
    "        assert sub_size <= inCh\n",
    "        self.sub_size = sub_size\n",
    "        self.inCh = inCh\n",
    "        self.outCh = outCh\n",
    "        self.kernel_height = kernel_size[0]\n",
    "        self.kernel_width = kernel_size[1]\n",
    "        \n",
    "        self.convs = nn.ParameterList([nn.Conv2d(self.sub_size, 1, kernel_size) for i in range(0, outCh)])\n",
    "        self.weights = torch.stack([i.weight for i in self.convs])\n",
    "        self.biases = torch.stack([i.bias for i in self.convs])\n",
    "        \n",
    "    def forward(self, X):\n",
    "        if len(X.shape) == 3:\n",
    "            X = X.unsqueeze(0)\n",
    "            \n",
    "            \n",
    "            \n",
    "        # Get the h/W output\n",
    "        h = X.shape[-2] - self.kernel_height\n",
    "        if self.kernel_height % 2 != 0:\n",
    "            h += 1\n",
    "        w = X.shape[-1] - self.kernel_width\n",
    "        if self.kernel_width % 2 != 0:\n",
    "            w += 1\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        # Number of desired channels\n",
    "        desired_channels = self.outCh+self.sub_size-1\n",
    "        # Number of times to repeat the tensor to get to that goal\n",
    "        num_repeats = math.ceil(desired_channels/self.inCh)\n",
    "        # Repeat the image num_repeats times along the channels\n",
    "        X = X.repeat(1, num_repeats, 1, 1)\n",
    "        # Slice the rest off that we don't need\n",
    "        X = X[:, :desired_channels]\n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "        # Pad the image by sub_size-1 along the channels to become (N, C+sub_size-1, L, W)\n",
    "        # X = torch.nn.functional.pad(input=X.unsqueeze(0), pad=(0,0,0,0,0,self.sub_size-1), mode=\"circular\").squeeze(0)\n",
    "        \n",
    "        # Unfold image (batch_size, channels+sub_size-1, windows, kernel_height, kernel_width)\n",
    "        X = X.unfold(2, self.kernel_height, 1).unfold(3, self.kernel_width, 1)\n",
    "        X = X.contiguous().view(X.shape[0], X.shape[1], -1, self.kernel_height, self.kernel_width)\n",
    "\n",
    "        # Let's unfold this tensor to be of shape (batch_size, outCh, windows, kernel_height, kernel_width, sub_size)\n",
    "        X = X.unfold(1, self.sub_size, 1)\n",
    "\n",
    "        # Make tensor of shape (batch_size, windows, outCh, sub_size, kernel_height, kernel_width)\n",
    "        X = X.permute(0, 2, 1, 5, 3, 4)\n",
    "\n",
    "        # Multiply the patches with the weights in order to calculate the conv (batch_size, outCh, HW)\n",
    "        X = (X * self.weights.transpose(0, 1).unsqueeze(0)).sum([3, 4, 5]).permute(0, 2, 1)\n",
    "        \n",
    "        # Add the biases\n",
    "        X += self.biases.unsqueeze(0)\n",
    "\n",
    "        # Reshape to output shape (batch_size, outCh, H, W)\n",
    "        return X.reshape(X.shape[0], -1, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8379ef5e-e4c4-434b-a512-71e79c0c2fcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# New conv - should be fast\n",
    "new_conv = Sparse_Conv_new(inCh, 1, (3, 3), sub_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6953cc98-1854-4d53-8fed-ae6a09e5d30d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Old conv - should be slow\n",
    "old_conv = Sparse_Conv_old(inCh, 1, (3, 3), sub_size)\n",
    "old_conv.convs = new_conv.convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5cc2db86-6d67-46ba-9fed-8c23115e447d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4 0\n"
     ]
    }
   ],
   "source": [
    "# Let's get the output for both\n",
    "new_out = new_conv(image)\n",
    "old_out = old_conv(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "387530d0-8258-438d-990b-b86b9e107768",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 10, 11])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e2e17337-937f-443b-a9e2-6bd8ce187398",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 10, 11])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "004beaab-e751-4349-86ad-199e16c14c68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1031,  0.0841, -0.2583, -0.3028, -0.5479, -0.5996, -0.2871, -0.4466,\n",
       "         -0.1664, -0.4724, -0.1575],\n",
       "        [-0.5450, -0.7332, -0.3128, -0.2300, -0.3033, -0.2435, -0.5466, -0.0620,\n",
       "         -0.1484, -0.2541, -0.2863],\n",
       "        [-0.3936, -0.2299, -0.0468, -0.0956, -0.2990, -0.3370, -0.3357, -0.2347,\n",
       "         -0.1475, -0.0404, -0.4263],\n",
       "        [-0.5244, -0.4245, -0.3017, -0.6112, -0.1812, -0.4746, -0.2088, -0.2968,\n",
       "         -0.6251, -0.1962, -0.3898],\n",
       "        [-0.3357, -0.1350, -0.2825, -0.3889, -0.3026, -0.3162, -0.3972, -0.1892,\n",
       "         -0.3293,  0.0150, -0.2099],\n",
       "        [ 0.0513, -0.5709, -0.3394, -0.4296, -0.2635, -0.0886, -0.1979, -0.3455,\n",
       "         -0.2843, -0.2960, -0.1861],\n",
       "        [-0.3702, -0.5569, -0.2921, -0.0489, -0.5124, -0.4308, -0.3535, -0.3309,\n",
       "         -0.2642, -0.1549, -0.3898],\n",
       "        [-0.1592, -0.1268, -0.2140, -0.5149, -0.6209, -0.5350, -0.2412, -0.3363,\n",
       "         -0.3722, -0.5017, -0.3613],\n",
       "        [-0.0909, -0.2174, -0.5810, -0.6015, -0.3517, -0.4718, -0.3015, -0.2051,\n",
       "         -0.3785, -0.1775, -0.4079],\n",
       "        [-0.3291, -0.3450, -0.2786, -0.3617, -0.3344, -0.2756, -0.1530, -0.1175,\n",
       "         -0.1076, -0.0720, -0.3299]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_out[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "337d861b-744d-4d4e-807e-b3fc921268ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1031,  0.0841, -0.2583, -0.3028, -0.5479, -0.5996, -0.2871, -0.4466,\n",
       "         -0.1664, -0.4724, -0.1575],\n",
       "        [-0.5450, -0.7332, -0.3128, -0.2300, -0.3033, -0.2435, -0.5466, -0.0620,\n",
       "         -0.1484, -0.2541, -0.2863],\n",
       "        [-0.3936, -0.2299, -0.0468, -0.0956, -0.2990, -0.3370, -0.3357, -0.2347,\n",
       "         -0.1475, -0.0404, -0.4263],\n",
       "        [-0.5244, -0.4245, -0.3017, -0.6112, -0.1812, -0.4746, -0.2088, -0.2968,\n",
       "         -0.6251, -0.1962, -0.3898],\n",
       "        [-0.3357, -0.1350, -0.2825, -0.3889, -0.3026, -0.3162, -0.3972, -0.1892,\n",
       "         -0.3293,  0.0150, -0.2099],\n",
       "        [ 0.0513, -0.5709, -0.3394, -0.4296, -0.2635, -0.0886, -0.1979, -0.3455,\n",
       "         -0.2843, -0.2960, -0.1861],\n",
       "        [-0.3702, -0.5569, -0.2921, -0.0489, -0.5124, -0.4308, -0.3535, -0.3309,\n",
       "         -0.2642, -0.1549, -0.3898],\n",
       "        [-0.1592, -0.1268, -0.2140, -0.5149, -0.6209, -0.5350, -0.2412, -0.3363,\n",
       "         -0.3722, -0.5017, -0.3613],\n",
       "        [-0.0909, -0.2174, -0.5810, -0.6015, -0.3517, -0.4718, -0.3015, -0.2051,\n",
       "         -0.3785, -0.1775, -0.4079],\n",
       "        [-0.3291, -0.3450, -0.2786, -0.3617, -0.3344, -0.2756, -0.1530, -0.1175,\n",
       "         -0.1076, -0.0720, -0.3299]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_out[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5a947ba6-5be2-45ad-bc18-f1f843e6fd97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Nice! Looks like it's working for less channels. What out more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9a6640-6efe-4da8-9ad5-03c4b534c61e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fdcb76-e515-4d35-8ce9-567e67943bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ab5412-a54c-4601-bfb4-92036cabf1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ebec53-889a-47e6-99c9-f7b02906c80a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8239e109-6440-4d67-89f4-3a13d201e51c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "69c5e396-e6fe-4d7a-a9cf-1b3eee2d1e6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Trying more than 1 channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "81b0bf36-9067-4ca5-9d66-742f626c8ebf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# New conv - should be fast\n",
    "new_conv = Sparse_Conv_new(inCh, 13, (3, 3), sub_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6ce3d900-f291-4c5b-8297-3b2c6b4662cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Old conv - should be slow\n",
    "old_conv = Sparse_Conv_old(inCh, 13, (3, 3), sub_size)\n",
    "old_conv.convs = new_conv.convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3898f8a7-b42b-49a3-a93d-c0db24085166",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4 0\n",
      "0 4 0\n",
      "1 5 0\n",
      "2 6 0\n",
      "3 7 0\n",
      "3 7 0\n",
      "4 8 0\n",
      "5 9 0\n",
      "6 10 0\n",
      "6 10 0\n",
      "7 10 1\n",
      "8 10 2\n",
      "9 10 3\n"
     ]
    }
   ],
   "source": [
    "# Let's get the output for both\n",
    "new_out = new_conv(image)\n",
    "old_out = old_conv(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "84abf4fa-d4e0-4861-beab-bf88fce8e02c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 10, 11])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "75d79de2-fd6d-46e8-ac06-0af22d50ca63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 10, 11])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "83bf35fc-8447-4e63-aadf-003d172ce039",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5026, -0.5090, -0.7384, -0.7571, -0.4243, -0.4055, -0.2884, -0.6291,\n",
       "        -0.3119, -0.2803, -0.6103], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_out[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0a4b8071-041d-4dea-b099-71b0f60d2290",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5026, -0.5090, -0.7384, -0.7571, -0.4243, -0.4055, -0.2884, -0.6291,\n",
       "        -0.3119, -0.2803, -0.6103], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_out[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "dd2e21ef-a15c-4fd7-a2a8-5f1648b2aa93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note that the 1st convolution in old_out is the inCh+1th one in new_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8ccab33c-55e0-4a59-835d-d99f53907d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3534, -0.0025,  0.2519,  0.1554,  0.1382, -0.2893, -0.1240, -0.0676,\n",
       "        -0.0118,  0.4416, -0.0767], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_out[0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b3339a54-bd13-4b8b-b313-3daf86d4c4be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1063, -0.0421,  0.0628,  0.0503, -0.2039, -0.1631,  0.1163,  0.2855,\n",
       "         0.0290, -0.1114,  0.0829], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_out[0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "3b210d09-52a1-45de-82bf-43dfd74326ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EH whatever good enough :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bd1cace0-a69c-486c-a7a5-3e8d399ea96a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_conv_ = Sparse_Conv_new(inCh, 1, (3, 3), sub_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b40b6486-a4dc-4471-83fc-b3030c94f6fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1063, -0.0421,  0.0628,  0.0503, -0.2039, -0.1631,  0.1163,\n",
       "            0.2855,  0.0290, -0.1114,  0.0829],\n",
       "          [ 0.0875,  0.0832, -0.0584,  0.0506,  0.0688,  0.1033,  0.2276,\n",
       "           -0.1613,  0.1657,  0.2156, -0.3551],\n",
       "          [-0.1080, -0.1465, -0.2105, -0.4055,  0.3021, -0.0491, -0.0357,\n",
       "           -0.0458,  0.0398,  0.1756,  0.1060],\n",
       "          [-0.0041,  0.1989,  0.2022,  0.1812, -0.0950, -0.0478,  0.0589,\n",
       "            0.1771, -0.2062, -0.3233, -0.2548],\n",
       "          [-0.0601, -0.1583,  0.0608,  0.2234, -0.0060, -0.0835,  0.2972,\n",
       "           -0.0902, -0.1343,  0.0225,  0.0680],\n",
       "          [-0.1732, -0.0916,  0.0513, -0.0832,  0.1527,  0.2315,  0.3204,\n",
       "           -0.3288, -0.2228,  0.0451, -0.0672],\n",
       "          [ 0.0706,  0.1927,  0.2232, -0.2133,  0.0987,  0.0525, -0.2881,\n",
       "           -0.3337,  0.0343,  0.3164,  0.3772],\n",
       "          [-0.1197, -0.0913, -0.0454, -0.0999, -0.0718, -0.0934, -0.0471,\n",
       "            0.2693,  0.0440, -0.0263, -0.0289],\n",
       "          [-0.0039,  0.1303,  0.0942, -0.0648,  0.0224,  0.2683,  0.2234,\n",
       "           -0.1095,  0.0564,  0.1811,  0.3609],\n",
       "          [-0.0387,  0.1751, -0.1319,  0.1111,  0.1444, -0.0013, -0.0216,\n",
       "           -0.1622,  0.0180, -0.0563,  0.0141]]]],\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_conv_(image[: :1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9933c038-b2e0-4750-89ea-8d6ec7aea2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
