{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117886d1-e8c1-4418-9d20-a00c090310a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5f9f954-7ea8-49c3-ae59-e7b840fe1ac9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used to transform the data to a transor\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        # Transform to a tensor\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec9ff65-021d-4fc4-9f85-dba43134939e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb8f94e5-7771-43d3-90c3-edc610f72477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now let's try this with multiple output channels and a\n",
    "# subset of inputs channels\n",
    "class Sparse_Conv(nn.Module):\n",
    "    def __init__(self, inCh, outCh, kernel_size, sub_size):\n",
    "        super(Sparse_Conv, self).__init__()\n",
    "        \n",
    "        assert sub_size <= inCh\n",
    "        self.sub_size = sub_size\n",
    "        self.inCh = inCh\n",
    "        self.outCh = outCh\n",
    "        self.kernel_height = kernel_size[0]\n",
    "        self.kernel_width = kernel_size[1]\n",
    "        \n",
    "        self.convs = nn.ParameterList([nn.Conv2d(self.sub_size, 1, kernel_size) for i in range(0, outCh)])\n",
    "        self.weights = torch.stack([i.weight for i in self.convs])\n",
    "        self.biases = torch.stack([i.bias for i in self.convs])\n",
    "        \n",
    "    def forward(self, X):\n",
    "        if len(X.shape) == 3:\n",
    "            X = X.unsqueeze(0)\n",
    "            \n",
    "            \n",
    "            \n",
    "        # Get the h/W output\n",
    "        h = X.shape[-2] - self.kernel_height\n",
    "        if self.kernel_height % 2 != 0:\n",
    "            h += 1\n",
    "        w = X.shape[-1] - self.kernel_width\n",
    "        if self.kernel_width % 2 != 0:\n",
    "            w += 1\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        # Number of desired channels\n",
    "        desired_channels = self.outCh+self.sub_size-1\n",
    "        # Number of times to repeat the tensor to get to that goal\n",
    "        num_repeats = math.ceil(desired_channels/self.inCh)\n",
    "        # Repeat the image num_repeats times along the channels\n",
    "        X = X.repeat(1, num_repeats, 1, 1)\n",
    "        # Slice the rest off that we don't need\n",
    "        X = X[:, :desired_channels]\n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "        # Pad the image by sub_size-1 along the channels to become (N, C+sub_size-1, L, W)\n",
    "        # X = torch.nn.functional.pad(input=X.unsqueeze(0), pad=(0,0,0,0,0,self.sub_size-1), mode=\"circular\").squeeze(0)\n",
    "        \n",
    "        # Unfold image (batch_size, channels+sub_size-1, windows, kernel_height, kernel_width)\n",
    "        X = X.unfold(2, self.kernel_height, 1).unfold(3, self.kernel_width, 1)\n",
    "        X = X.contiguous().view(X.shape[0], X.shape[1], -1, self.kernel_height, self.kernel_width)\n",
    "\n",
    "        # Let's unfold this tensor to be of shape (batch_size, outCh, windows, kernel_height, kernel_width, sub_size)\n",
    "        X = X.unfold(1, self.sub_size, 1)\n",
    "\n",
    "        # Make tensor of shape (batch_size, windows, outCh, sub_size, kernel_height, kernel_width)\n",
    "        X = X.permute(0, 2, 1, 5, 3, 4)\n",
    "\n",
    "        # Multiply the patches with the weights in order to calculate the conv (batch_size, outCh, HW)\n",
    "        X = (X * self.weights.transpose(0, 1).unsqueeze(0)).sum([3, 4, 5]).permute(0, 2, 1)\n",
    "        \n",
    "        # Add the biases\n",
    "        X += self.biases.unsqueeze(0)\n",
    "\n",
    "        # Reshape to output shape (batch_size, outCh, H, W)\n",
    "        return X.reshape(X.shape[0], -1, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac1d8e3-34c1-4d70-a301-89864534f0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4775b9cb-dcb7-48eb-8efa-6bc9cebe6490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a511561f-f276-4d22-92b5-688f26d0aa49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b264802f-4b72-4a52-89b2-5acca27b58a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f57369a-9599-4d67-89c1-e57a174bbbff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load in MNIST\n",
    "MNIST_dataset = torchvision.datasets.MNIST(\"./\", train=True, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca09de69-c08a-4387-babf-6eee0551dda7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used to load in the dataset\n",
    "data_loader = DataLoader(MNIST_dataset, batch_size=64,\n",
    "        pin_memory=True, num_workers=1, \n",
    "        drop_last=False, shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d4e571be-f29e-4bd4-b68d-2cf5c8640126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model with 1x28x28 input and 10 output\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # convolution layers\n",
    "        self.convs = nn.Sequential( # 1x28x28\n",
    "            Sparse_Conv(1, 32, (3, 3), 1), # 32x26x26\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            Sparse_Conv(32, 64, (3, 3), 5), # 64x24x24\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3), # 64x8x8\n",
    "            \n",
    "            nn.Flatten(1, -1), # 4096\n",
    "            nn.Linear(4096, 250), # 250\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(250, 10),\n",
    "            nn.Softmax(-1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.convs(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b863eb67-e3b8-4522-8e70-6629fc604235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9aab1a05-cfd8-4430-beb0-9b22eef3bde5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optim = torch.optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e62526e0-6c3a-42b9-879d-693d78bd642b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss_funct = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c7a5e2-11a5-4983-9600-a2557cf71dea",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: 2.30289888381958\n",
      "step 2: 2.298799514770508\n",
      "step 3: 2.3022468090057373\n",
      "step 4: 2.291330099105835\n",
      "step 5: 2.27962589263916\n",
      "step 6: 2.2782809734344482\n",
      "step 7: 2.261451005935669\n",
      "step 8: 2.2630674839019775\n",
      "step 9: 2.253166437149048\n",
      "step 10: 2.2540359497070312\n",
      "step 11: 2.262906074523926\n",
      "step 12: 2.2558646202087402\n",
      "step 13: 2.2808828353881836\n",
      "step 14: 2.2111637592315674\n",
      "step 15: 2.1603310108184814\n",
      "step 16: 2.207650899887085\n",
      "step 17: 2.1825063228607178\n",
      "step 18: 2.16615629196167\n",
      "step 19: 2.1009445190429688\n",
      "step 20: 2.1772756576538086\n",
      "step 21: 2.134047746658325\n",
      "step 22: 2.1929731369018555\n",
      "step 23: 2.1163365840911865\n",
      "step 24: 2.106511354446411\n",
      "step 25: 2.0999929904937744\n",
      "step 26: 2.1082231998443604\n",
      "step 27: 2.0805699825286865\n",
      "step 28: 2.06715726852417\n",
      "step 29: 2.067384958267212\n",
      "step 30: 2.1241672039031982\n",
      "step 31: 2.0837721824645996\n",
      "step 32: 2.1100542545318604\n",
      "step 33: 2.045210838317871\n",
      "step 34: 2.0776586532592773\n",
      "step 35: 2.0383312702178955\n",
      "step 36: 2.012605667114258\n",
      "step 37: 1.9717931747436523\n",
      "step 38: 2.054450750350952\n",
      "step 39: 2.0515010356903076\n",
      "step 40: 1.980347752571106\n",
      "step 41: 2.0570828914642334\n",
      "step 42: 2.014842987060547\n",
      "step 43: 2.0045974254608154\n",
      "step 44: 1.9306329488754272\n",
      "step 45: 2.01650333404541\n",
      "step 46: 2.0241599082946777\n",
      "step 47: 1.9261237382888794\n",
      "step 48: 1.959863305091858\n",
      "step 49: 1.9310314655303955\n",
      "step 50: 1.8987253904342651\n",
      "step 51: 1.9610133171081543\n",
      "step 52: 1.8980921506881714\n",
      "step 53: 1.9297620058059692\n",
      "step 54: 1.962173581123352\n",
      "step 55: 1.980430006980896\n",
      "step 56: 1.8968256711959839\n",
      "step 57: 1.9330434799194336\n",
      "step 58: 1.8805906772613525\n",
      "step 59: 1.908818006515503\n",
      "step 60: 1.8512929677963257\n",
      "step 61: 1.8885489702224731\n",
      "step 62: 1.9254233837127686\n",
      "step 63: 1.9042537212371826\n",
      "step 64: 1.8711360692977905\n",
      "step 65: 1.9006954431533813\n",
      "step 66: 1.9615232944488525\n",
      "step 67: 1.8697116374969482\n",
      "step 68: 1.8584926128387451\n",
      "step 69: 1.8511829376220703\n",
      "step 70: 1.8330883979797363\n",
      "step 71: 1.8866477012634277\n",
      "step 72: 1.784666895866394\n",
      "step 73: 1.9112640619277954\n",
      "step 74: 1.8269658088684082\n",
      "step 75: 1.9010907411575317\n",
      "step 76: 1.8775928020477295\n",
      "step 77: 1.8682960271835327\n",
      "step 78: 1.8831697702407837\n",
      "step 79: 1.8602218627929688\n",
      "step 80: 1.893288493156433\n",
      "step 81: 1.8245713710784912\n",
      "step 82: 1.8083767890930176\n",
      "step 83: 1.767914056777954\n",
      "step 84: 1.8164912462234497\n",
      "step 85: 1.7755786180496216\n",
      "step 86: 1.8349801301956177\n",
      "step 87: 1.7907702922821045\n",
      "step 88: 1.8517411947250366\n",
      "step 89: 1.753295660018921\n",
      "step 90: 1.7608259916305542\n",
      "step 91: 1.831046223640442\n",
      "step 92: 1.7869316339492798\n",
      "step 93: 1.7151471376419067\n",
      "step 94: 1.7886549234390259\n",
      "step 95: 1.827663540840149\n",
      "step 96: 1.7810227870941162\n",
      "step 97: 1.8910043239593506\n",
      "step 98: 1.74015474319458\n",
      "step 99: 1.7432767152786255\n",
      "step 100: 1.7757465839385986\n",
      "step 101: 1.7668646574020386\n",
      "step 102: 1.7167266607284546\n",
      "step 103: 1.7980780601501465\n",
      "step 104: 1.7511202096939087\n",
      "step 105: 1.8039263486862183\n",
      "step 106: 1.6693525314331055\n",
      "step 107: 1.8016785383224487\n",
      "step 108: 1.8431841135025024\n",
      "step 109: 1.736844539642334\n",
      "step 110: 1.8219413757324219\n",
      "step 111: 1.7953263521194458\n",
      "step 112: 1.841255784034729\n",
      "step 113: 1.7444448471069336\n",
      "step 114: 1.6984597444534302\n",
      "step 115: 1.7526204586029053\n",
      "step 116: 1.7694069147109985\n",
      "step 117: 1.7341936826705933\n",
      "step 118: 1.7207098007202148\n",
      "step 119: 1.783929467201233\n",
      "step 120: 1.6949453353881836\n",
      "step 121: 1.7050256729125977\n",
      "step 122: 1.666743516921997\n",
      "step 123: 1.7398139238357544\n",
      "step 124: 1.7177774906158447\n",
      "step 125: 1.746314525604248\n",
      "step 126: 1.7106090784072876\n",
      "step 127: 1.6703400611877441\n",
      "step 128: 1.7895190715789795\n",
      "step 129: 1.7188405990600586\n",
      "step 130: 1.7085914611816406\n",
      "step 131: 1.729875087738037\n",
      "step 132: 1.7174164056777954\n",
      "step 133: 1.7036572694778442\n",
      "step 134: 1.7069164514541626\n",
      "step 135: 1.7068850994110107\n",
      "step 136: 1.733134150505066\n",
      "step 137: 1.7437591552734375\n",
      "step 138: 1.7035596370697021\n",
      "step 139: 1.6296453475952148\n",
      "step 140: 1.693468451499939\n",
      "step 141: 1.702987551689148\n",
      "step 142: 1.6851798295974731\n",
      "step 143: 1.627997636795044\n",
      "step 144: 1.6539318561553955\n",
      "step 145: 1.6267940998077393\n",
      "step 146: 1.7122985124588013\n",
      "step 147: 1.7321293354034424\n",
      "step 148: 1.6713271141052246\n",
      "step 149: 1.6911823749542236\n",
      "step 150: 1.6772743463516235\n",
      "step 151: 1.6539247035980225\n",
      "step 152: 1.7208049297332764\n",
      "step 153: 1.664238691329956\n",
      "step 154: 1.6561415195465088\n",
      "step 155: 1.7191637754440308\n",
      "step 156: 1.6228079795837402\n",
      "step 157: 1.668338418006897\n",
      "step 158: 1.674821138381958\n",
      "step 159: 1.6987799406051636\n",
      "step 160: 1.6201385259628296\n",
      "step 161: 1.6389859914779663\n",
      "step 162: 1.6827194690704346\n",
      "step 163: 1.6464389562606812\n",
      "step 164: 1.5974305868148804\n",
      "step 165: 1.6124334335327148\n",
      "step 166: 1.629215121269226\n",
      "step 167: 1.6291537284851074\n",
      "step 168: 1.6138619184494019\n",
      "step 169: 1.5930055379867554\n",
      "step 170: 1.6502619981765747\n",
      "step 171: 1.5842081308364868\n",
      "step 172: 1.6606439352035522\n",
      "step 173: 1.6393293142318726\n",
      "step 174: 1.614371418952942\n",
      "step 175: 1.6269190311431885\n",
      "step 176: 1.6787115335464478\n",
      "step 177: 1.616056203842163\n",
      "step 178: 1.606667160987854\n",
      "step 179: 1.5936030149459839\n",
      "step 180: 1.6370400190353394\n",
      "step 181: 1.6045923233032227\n",
      "step 182: 1.6262437105178833\n",
      "step 183: 1.6040217876434326\n",
      "step 184: 1.6050400733947754\n",
      "step 185: 1.6318995952606201\n",
      "step 186: 1.7036479711532593\n",
      "step 187: 1.636860728263855\n",
      "step 188: 1.6271501779556274\n",
      "step 189: 1.6558114290237427\n",
      "step 190: 1.5938682556152344\n",
      "step 191: 1.6018354892730713\n",
      "step 192: 1.6405755281448364\n",
      "step 193: 1.6321756839752197\n",
      "step 194: 1.6516786813735962\n",
      "step 195: 1.574376106262207\n",
      "step 196: 1.7112704515457153\n",
      "step 197: 1.5861570835113525\n",
      "step 198: 1.5764390230178833\n",
      "step 199: 1.5840530395507812\n",
      "step 200: 1.5935736894607544\n",
      "step 201: 1.5631968975067139\n",
      "step 202: 1.689652681350708\n",
      "step 203: 1.6169123649597168\n",
      "step 204: 1.631539225578308\n",
      "step 205: 1.6056721210479736\n",
      "step 206: 1.6451047658920288\n",
      "step 207: 1.5637620687484741\n",
      "step 208: 1.5818275213241577\n",
      "step 209: 1.6301343441009521\n",
      "step 210: 1.6051037311553955\n",
      "step 211: 1.639845371246338\n",
      "step 212: 1.6476073265075684\n",
      "step 213: 1.6354185342788696\n",
      "step 214: 1.6001590490341187\n",
      "step 215: 1.5889718532562256\n",
      "step 216: 1.644911289215088\n",
      "step 217: 1.6010059118270874\n",
      "step 218: 1.5906825065612793\n",
      "step 219: 1.607962727546692\n",
      "step 220: 1.6031144857406616\n",
      "step 221: 1.6225934028625488\n",
      "step 222: 1.5992497205734253\n",
      "step 223: 1.5634254217147827\n",
      "step 224: 1.6359448432922363\n",
      "step 225: 1.5437942743301392\n",
      "step 226: 1.6256434917449951\n",
      "step 227: 1.5844964981079102\n",
      "step 228: 1.5865901708602905\n",
      "step 229: 1.5865484476089478\n",
      "step 230: 1.640749454498291\n",
      "step 231: 1.6039718389511108\n",
      "step 232: 1.6346361637115479\n",
      "step 233: 1.6093952655792236\n",
      "step 234: 1.6243425607681274\n",
      "step 235: 1.5926780700683594\n",
      "step 236: 1.653016448020935\n",
      "step 237: 1.5991613864898682\n",
      "step 238: 1.582963466644287\n",
      "step 239: 1.5800706148147583\n",
      "step 240: 1.605892300605774\n",
      "step 241: 1.6129149198532104\n",
      "step 242: 1.5733391046524048\n",
      "step 243: 1.6472134590148926\n",
      "step 244: 1.6093382835388184\n",
      "step 245: 1.6108546257019043\n",
      "step 246: 1.6084940433502197\n",
      "step 247: 1.6037096977233887\n",
      "step 248: 1.6026917695999146\n",
      "step 249: 1.6394436359405518\n",
      "step 250: 1.621787428855896\n",
      "step 251: 1.6142445802688599\n",
      "step 252: 1.6239842176437378\n",
      "step 253: 1.6388777494430542\n",
      "step 254: 1.6101391315460205\n",
      "step 255: 1.6183029413223267\n",
      "step 256: 1.585970401763916\n",
      "step 257: 1.5884672403335571\n",
      "step 258: 1.5738359689712524\n",
      "step 259: 1.540911078453064\n",
      "step 260: 1.7134590148925781\n",
      "step 261: 1.6085656881332397\n",
      "step 262: 1.6385856866836548\n",
      "step 263: 1.581781029701233\n",
      "step 264: 1.5915250778198242\n",
      "step 265: 1.6431524753570557\n",
      "step 266: 1.6043976545333862\n",
      "step 267: 1.542405605316162\n",
      "step 268: 1.5823556184768677\n",
      "step 269: 1.5822196006774902\n",
      "step 270: 1.6397258043289185\n",
      "step 271: 1.6038949489593506\n",
      "step 272: 1.5696662664413452\n",
      "step 273: 1.6104992628097534\n",
      "step 274: 1.5585134029388428\n",
      "step 275: 1.6381255388259888\n",
      "step 276: 1.5959256887435913\n",
      "step 277: 1.6499357223510742\n",
      "step 278: 1.5850099325180054\n",
      "step 279: 1.6159287691116333\n",
      "step 280: 1.5443459749221802\n",
      "step 281: 1.6021127700805664\n",
      "step 282: 1.5763037204742432\n",
      "step 283: 1.6257888078689575\n",
      "step 284: 1.5700842142105103\n",
      "step 285: 1.6084990501403809\n",
      "step 286: 1.597216010093689\n",
      "step 287: 1.563944935798645\n",
      "step 288: 1.526915431022644\n",
      "step 289: 1.6072696447372437\n",
      "step 290: 1.5652174949645996\n",
      "step 291: 1.5996712446212769\n",
      "step 292: 1.5637474060058594\n",
      "step 293: 1.5654901266098022\n",
      "step 294: 1.5757288932800293\n",
      "step 295: 1.5877275466918945\n",
      "step 296: 1.5953736305236816\n",
      "step 297: 1.5847541093826294\n",
      "step 298: 1.6346681118011475\n",
      "step 299: 1.5932979583740234\n",
      "step 300: 1.6111023426055908\n",
      "step 301: 1.5597845315933228\n",
      "step 302: 1.592266321182251\n",
      "step 303: 1.5905735492706299\n",
      "step 304: 1.610276460647583\n",
      "step 305: 1.5717408657073975\n",
      "step 306: 1.556682825088501\n",
      "step 307: 1.5560579299926758\n",
      "step 308: 1.547298550605774\n",
      "step 309: 1.5337045192718506\n",
      "step 310: 1.578547477722168\n",
      "step 311: 1.620194911956787\n",
      "step 312: 1.544692039489746\n",
      "step 313: 1.587235689163208\n",
      "step 314: 1.584931492805481\n",
      "step 315: 1.5569959878921509\n",
      "step 316: 1.5439296960830688\n",
      "step 317: 1.592952013015747\n",
      "step 318: 1.517012119293213\n",
      "step 319: 1.59468674659729\n",
      "step 320: 1.5920989513397217\n",
      "step 321: 1.575634241104126\n",
      "step 322: 1.6105648279190063\n",
      "step 323: 1.5624804496765137\n",
      "step 324: 1.538130521774292\n",
      "step 325: 1.6010453701019287\n",
      "step 326: 1.574731707572937\n",
      "step 327: 1.5689014196395874\n",
      "step 328: 1.6018975973129272\n",
      "step 329: 1.5445672273635864\n",
      "step 330: 1.576558232307434\n",
      "step 331: 1.5294673442840576\n",
      "step 332: 1.549201488494873\n",
      "step 333: 1.5848135948181152\n",
      "step 334: 1.549408197402954\n",
      "step 335: 1.5381184816360474\n",
      "step 336: 1.5734447240829468\n",
      "step 337: 1.5425034761428833\n",
      "step 338: 1.6025304794311523\n",
      "step 339: 1.5846805572509766\n",
      "step 340: 1.5747102499008179\n",
      "step 341: 1.5954041481018066\n",
      "step 342: 1.6017587184906006\n",
      "step 343: 1.5851001739501953\n",
      "step 344: 1.593654751777649\n",
      "step 345: 1.5522223711013794\n",
      "step 346: 1.5717970132827759\n",
      "step 347: 1.5346018075942993\n",
      "step 348: 1.5674391984939575\n",
      "step 349: 1.6077847480773926\n",
      "step 350: 1.572487473487854\n",
      "step 351: 1.584484577178955\n",
      "step 352: 1.561431884765625\n",
      "step 353: 1.5387383699417114\n",
      "step 354: 1.5274498462677002\n",
      "step 355: 1.5653579235076904\n",
      "step 356: 1.560797929763794\n",
      "step 357: 1.5622550249099731\n",
      "step 358: 1.5830157995224\n",
      "step 359: 1.5870603322982788\n",
      "step 360: 1.5882165431976318\n",
      "step 361: 1.5205579996109009\n",
      "step 362: 1.5509825944900513\n",
      "step 363: 1.5317001342773438\n",
      "step 364: 1.5797314643859863\n",
      "step 365: 1.582658052444458\n",
      "step 366: 1.6053396463394165\n",
      "step 367: 1.5823501348495483\n",
      "step 368: 1.5719857215881348\n",
      "step 369: 1.600851058959961\n",
      "step 370: 1.592565894126892\n",
      "step 371: 1.5761761665344238\n",
      "step 372: 1.5587725639343262\n",
      "step 373: 1.5685932636260986\n",
      "step 374: 1.5973538160324097\n",
      "step 375: 1.53372061252594\n",
      "step 376: 1.5794477462768555\n",
      "step 377: 1.574888825416565\n",
      "step 378: 1.5465501546859741\n",
      "step 379: 1.5504716634750366\n",
      "step 380: 1.5828415155410767\n",
      "step 381: 1.5820095539093018\n",
      "step 382: 1.575061559677124\n",
      "step 383: 1.5828644037246704\n",
      "step 384: 1.5330150127410889\n",
      "step 385: 1.5338627099990845\n",
      "step 386: 1.5796853303909302\n",
      "step 387: 1.5501248836517334\n",
      "step 388: 1.5680601596832275\n",
      "step 389: 1.6175414323806763\n",
      "step 390: 1.5655326843261719\n",
      "step 391: 1.543699860572815\n",
      "step 392: 1.6086666584014893\n",
      "step 393: 1.6347273588180542\n",
      "step 394: 1.5610687732696533\n",
      "step 395: 1.5235874652862549\n",
      "step 396: 1.5898170471191406\n",
      "step 397: 1.5735384225845337\n",
      "step 398: 1.582430362701416\n",
      "step 399: 1.54389488697052\n",
      "step 400: 1.5502816438674927\n",
      "step 401: 1.545736312866211\n",
      "step 402: 1.5337380170822144\n",
      "step 403: 1.5477042198181152\n",
      "step 404: 1.538905143737793\n",
      "step 405: 1.5544894933700562\n",
      "step 406: 1.5794638395309448\n",
      "step 407: 1.5699743032455444\n",
      "step 408: 1.5403475761413574\n",
      "step 409: 1.6106466054916382\n",
      "step 410: 1.5998133420944214\n",
      "step 411: 1.5359630584716797\n",
      "step 412: 1.5113880634307861\n",
      "step 413: 1.577883243560791\n",
      "step 414: 1.5187721252441406\n",
      "step 415: 1.5838022232055664\n",
      "step 416: 1.5452913045883179\n",
      "step 417: 1.5537099838256836\n",
      "step 418: 1.5933955907821655\n",
      "step 419: 1.601814866065979\n",
      "step 420: 1.5726336240768433\n",
      "step 421: 1.5533838272094727\n",
      "step 422: 1.5397727489471436\n",
      "step 423: 1.5167566537857056\n",
      "step 424: 1.5493162870407104\n",
      "step 425: 1.5531495809555054\n",
      "step 426: 1.536471962928772\n",
      "step 427: 1.5644198656082153\n",
      "step 428: 1.5372790098190308\n",
      "step 429: 1.5477573871612549\n",
      "step 430: 1.5305418968200684\n",
      "step 431: 1.6177186965942383\n",
      "step 432: 1.6187998056411743\n",
      "step 433: 1.5763163566589355\n",
      "step 434: 1.5600605010986328\n",
      "step 435: 1.5781679153442383\n",
      "step 436: 1.6291793584823608\n",
      "step 437: 1.5452134609222412\n",
      "step 438: 1.5926612615585327\n",
      "step 439: 1.537869930267334\n",
      "step 440: 1.540166974067688\n",
      "step 441: 1.5882915258407593\n",
      "step 442: 1.553127408027649\n",
      "step 443: 1.5104061365127563\n",
      "step 444: 1.5839470624923706\n",
      "step 445: 1.5296862125396729\n",
      "step 446: 1.62478506565094\n",
      "step 447: 1.6017260551452637\n",
      "step 448: 1.5698058605194092\n",
      "step 449: 1.5248695611953735\n",
      "step 450: 1.5915510654449463\n",
      "step 451: 1.5429234504699707\n",
      "step 452: 1.545955777168274\n",
      "step 453: 1.542628288269043\n",
      "step 454: 1.595552921295166\n",
      "step 455: 1.5797699689865112\n",
      "step 456: 1.5031284093856812\n",
      "step 457: 1.617545247077942\n",
      "step 458: 1.5482125282287598\n",
      "step 459: 1.5307313203811646\n",
      "step 460: 1.551663875579834\n",
      "step 461: 1.5773577690124512\n",
      "step 462: 1.5878897905349731\n",
      "step 463: 1.520843505859375\n",
      "step 464: 1.5402384996414185\n",
      "step 465: 1.5790687799453735\n",
      "step 466: 1.5456842184066772\n",
      "step 467: 1.59320867061615\n",
      "step 468: 1.5266786813735962\n",
      "step 469: 1.5325007438659668\n",
      "step 470: 1.575843334197998\n",
      "step 471: 1.5564264059066772\n",
      "step 472: 1.556640386581421\n",
      "step 473: 1.5745450258255005\n",
      "step 474: 1.5253444910049438\n",
      "step 475: 1.5534145832061768\n",
      "step 476: 1.59641695022583\n",
      "step 477: 1.5733880996704102\n",
      "step 478: 1.5747700929641724\n",
      "step 479: 1.5534244775772095\n",
      "step 480: 1.5520249605178833\n",
      "step 481: 1.5624927282333374\n",
      "step 482: 1.5576974153518677\n",
      "step 483: 1.5155994892120361\n",
      "step 484: 1.5795531272888184\n",
      "step 485: 1.6048147678375244\n",
      "step 486: 1.5652437210083008\n",
      "step 487: 1.5513535737991333\n",
      "step 488: 1.5362218618392944\n",
      "step 489: 1.5437787771224976\n",
      "step 490: 1.5414681434631348\n",
      "step 491: 1.5309497117996216\n",
      "step 492: 1.5758565664291382\n",
      "step 493: 1.5574169158935547\n",
      "step 494: 1.6203052997589111\n",
      "step 495: 1.5434516668319702\n",
      "step 496: 1.5248866081237793\n",
      "step 497: 1.506955862045288\n",
      "step 498: 1.5590742826461792\n",
      "step 499: 1.5853488445281982\n",
      "step 500: 1.5387908220291138\n",
      "step 501: 1.5787022113800049\n",
      "step 502: 1.5270967483520508\n",
      "step 503: 1.5185604095458984\n",
      "step 504: 1.5779705047607422\n",
      "step 505: 1.5760427713394165\n",
      "step 506: 1.56649649143219\n",
      "step 507: 1.520938754081726\n",
      "step 508: 1.51752507686615\n",
      "step 509: 1.511393427848816\n",
      "step 510: 1.601658821105957\n",
      "step 511: 1.5748637914657593\n",
      "step 512: 1.5206506252288818\n",
      "step 513: 1.6000832319259644\n",
      "step 514: 1.5176891088485718\n",
      "step 515: 1.4934678077697754\n",
      "step 516: 1.5322924852371216\n",
      "step 517: 1.5182273387908936\n",
      "step 518: 1.5389153957366943\n",
      "step 519: 1.5025043487548828\n",
      "step 520: 1.5547131299972534\n",
      "step 521: 1.5185940265655518\n",
      "step 522: 1.5752679109573364\n",
      "step 523: 1.5440500974655151\n",
      "step 524: 1.5394207239151\n",
      "step 525: 1.5288499593734741\n",
      "step 526: 1.5328255891799927\n",
      "step 527: 1.5165456533432007\n",
      "step 528: 1.543952226638794\n",
      "step 529: 1.5470305681228638\n",
      "step 530: 1.5143166780471802\n",
      "step 531: 1.5809694528579712\n",
      "step 532: 1.5511637926101685\n",
      "step 533: 1.5169692039489746\n",
      "step 534: 1.531325101852417\n",
      "step 535: 1.5424096584320068\n",
      "step 536: 1.5493848323822021\n",
      "step 537: 1.5660040378570557\n",
      "step 538: 1.5772901773452759\n",
      "step 539: 1.560782551765442\n",
      "step 540: 1.567918062210083\n",
      "step 541: 1.5825697183609009\n",
      "step 542: 1.5790754556655884\n",
      "step 543: 1.5637352466583252\n",
      "step 544: 1.5650886297225952\n",
      "step 545: 1.5856772661209106\n",
      "step 546: 1.565171718597412\n",
      "step 547: 1.5537018775939941\n",
      "step 548: 1.5142076015472412\n",
      "step 549: 1.5532170534133911\n",
      "step 550: 1.540839672088623\n",
      "step 551: 1.5422208309173584\n",
      "step 552: 1.5063762664794922\n",
      "step 553: 1.5067906379699707\n",
      "step 554: 1.6016950607299805\n",
      "step 555: 1.506993293762207\n",
      "step 556: 1.532889723777771\n",
      "step 557: 1.5601787567138672\n",
      "step 558: 1.507994532585144\n",
      "step 559: 1.5250015258789062\n",
      "step 560: 1.5791090726852417\n",
      "step 561: 1.5672630071640015\n",
      "step 562: 1.59404718875885\n",
      "step 563: 1.5381734371185303\n",
      "step 564: 1.5285930633544922\n",
      "step 565: 1.519959807395935\n",
      "step 566: 1.579929232597351\n",
      "step 567: 1.5328991413116455\n",
      "step 568: 1.5317208766937256\n",
      "step 569: 1.5400614738464355\n",
      "step 570: 1.5738097429275513\n",
      "step 571: 1.5676628351211548\n",
      "step 572: 1.6032004356384277\n",
      "step 573: 1.5906293392181396\n",
      "step 574: 1.5284587144851685\n",
      "step 575: 1.5482027530670166\n",
      "step 576: 1.525848627090454\n",
      "step 577: 1.4905084371566772\n",
      "step 578: 1.5643045902252197\n",
      "step 579: 1.5320980548858643\n",
      "step 580: 1.5351157188415527\n",
      "step 581: 1.5416715145111084\n",
      "step 582: 1.568082571029663\n",
      "step 583: 1.5290242433547974\n",
      "step 584: 1.6222200393676758\n",
      "step 585: 1.5217194557189941\n",
      "step 586: 1.527066707611084\n",
      "step 587: 1.5119348764419556\n",
      "step 588: 1.5263656377792358\n",
      "step 589: 1.5029959678649902\n",
      "step 590: 1.5330415964126587\n",
      "step 591: 1.4885326623916626\n",
      "step 592: 1.5152547359466553\n",
      "step 593: 1.5219136476516724\n",
      "step 594: 1.5408614873886108\n",
      "step 595: 1.5788198709487915\n",
      "step 596: 1.515619158744812\n",
      "step 597: 1.5158588886260986\n",
      "step 598: 1.5125240087509155\n",
      "step 599: 1.5310947895050049\n",
      "step 600: 1.4908541440963745\n",
      "step 601: 1.5550596714019775\n",
      "step 602: 1.5691934823989868\n",
      "step 603: 1.5113697052001953\n",
      "step 604: 1.517777442932129\n",
      "step 605: 1.513081669807434\n",
      "step 606: 1.576480507850647\n",
      "step 607: 1.5346989631652832\n",
      "step 608: 1.5812078714370728\n",
      "step 609: 1.5539851188659668\n",
      "step 610: 1.5187665224075317\n",
      "step 611: 1.5289068222045898\n",
      "step 612: 1.496276617050171\n",
      "step 613: 1.5599229335784912\n",
      "step 614: 1.632506251335144\n",
      "step 615: 1.5400114059448242\n",
      "step 616: 1.555956244468689\n",
      "step 617: 1.5836114883422852\n",
      "step 618: 1.5230798721313477\n",
      "step 619: 1.5271176099777222\n",
      "step 620: 1.5715817213058472\n",
      "step 621: 1.5154062509536743\n",
      "step 622: 1.5469398498535156\n",
      "step 623: 1.5544954538345337\n",
      "step 624: 1.5138062238693237\n",
      "step 625: 1.5379605293273926\n",
      "step 626: 1.5709820985794067\n",
      "step 627: 1.562081217765808\n",
      "step 628: 1.558376431465149\n",
      "step 629: 1.5355523824691772\n",
      "step 630: 1.5185281038284302\n",
      "step 631: 1.5573248863220215\n",
      "step 632: 1.5797481536865234\n",
      "step 633: 1.5804780721664429\n",
      "step 634: 1.4867783784866333\n",
      "step 635: 1.531930923461914\n",
      "step 636: 1.5138376951217651\n",
      "step 637: 1.512807846069336\n",
      "step 638: 1.5424976348876953\n",
      "step 639: 1.5290461778640747\n",
      "step 640: 1.5642201900482178\n",
      "step 641: 1.5545490980148315\n",
      "step 642: 1.5051827430725098\n",
      "step 643: 1.5671478509902954\n",
      "step 644: 1.5757334232330322\n",
      "step 645: 1.5741852521896362\n",
      "step 646: 1.5219829082489014\n",
      "step 647: 1.5628933906555176\n",
      "step 648: 1.4999769926071167\n",
      "step 649: 1.5194438695907593\n",
      "step 650: 1.4970123767852783\n",
      "step 651: 1.5138239860534668\n",
      "step 652: 1.506737232208252\n",
      "step 653: 1.5992032289505005\n",
      "step 654: 1.6065956354141235\n",
      "step 655: 1.5338037014007568\n",
      "step 656: 1.5305010080337524\n",
      "step 657: 1.570717215538025\n",
      "step 658: 1.5409895181655884\n",
      "step 659: 1.524454951286316\n",
      "step 660: 1.5979688167572021\n",
      "step 661: 1.5346922874450684\n",
      "step 662: 1.4870415925979614\n",
      "step 663: 1.5249887704849243\n",
      "step 664: 1.6104633808135986\n",
      "step 665: 1.5136539936065674\n",
      "step 666: 1.5360056161880493\n",
      "step 667: 1.535652995109558\n",
      "step 668: 1.520699143409729\n",
      "step 669: 1.5529786348342896\n",
      "step 670: 1.5422166585922241\n",
      "step 671: 1.497422218322754\n",
      "step 672: 1.5222489833831787\n",
      "step 673: 1.5267529487609863\n",
      "step 674: 1.5494368076324463\n",
      "step 675: 1.490189552307129\n",
      "step 676: 1.5174375772476196\n",
      "step 677: 1.5209331512451172\n",
      "step 678: 1.559644103050232\n",
      "step 679: 1.5596741437911987\n",
      "step 680: 1.5397253036499023\n",
      "step 681: 1.5881768465042114\n",
      "step 682: 1.4849404096603394\n",
      "step 683: 1.5129839181900024\n",
      "step 684: 1.4715524911880493\n",
      "step 685: 1.5147634744644165\n",
      "step 686: 1.5077977180480957\n",
      "step 687: 1.5719271898269653\n",
      "step 688: 1.5725362300872803\n",
      "step 689: 1.5327444076538086\n",
      "step 690: 1.5521278381347656\n",
      "step 691: 1.5088307857513428\n",
      "step 692: 1.5460963249206543\n",
      "step 693: 1.543593406677246\n",
      "step 694: 1.613162875175476\n",
      "step 695: 1.5237798690795898\n",
      "step 696: 1.4929043054580688\n",
      "step 697: 1.5216110944747925\n",
      "step 698: 1.5501222610473633\n",
      "step 699: 1.5332783460617065\n",
      "step 700: 1.5509532690048218\n",
      "step 701: 1.494400143623352\n",
      "step 702: 1.546613335609436\n",
      "step 703: 1.5153712034225464\n",
      "step 704: 1.511985421180725\n",
      "step 705: 1.529807686805725\n",
      "step 706: 1.5439659357070923\n",
      "step 707: 1.5812634229660034\n",
      "step 708: 1.5327434539794922\n",
      "step 709: 1.4986085891723633\n",
      "step 710: 1.5625735521316528\n",
      "step 711: 1.5378309488296509\n",
      "step 712: 1.5470545291900635\n",
      "step 713: 1.5803955793380737\n",
      "step 714: 1.5463439226150513\n",
      "step 715: 1.5229259729385376\n",
      "step 716: 1.5393474102020264\n",
      "step 717: 1.5433286428451538\n",
      "step 718: 1.5295964479446411\n",
      "step 719: 1.5213791131973267\n",
      "step 720: 1.5805777311325073\n",
      "step 721: 1.5347298383712769\n",
      "step 722: 1.545652151107788\n",
      "step 723: 1.5559515953063965\n",
      "step 724: 1.5293290615081787\n",
      "step 725: 1.5634844303131104\n",
      "step 726: 1.5915499925613403\n",
      "step 727: 1.5342144966125488\n",
      "step 728: 1.5788496732711792\n",
      "step 729: 1.5549874305725098\n",
      "step 730: 1.5352476835250854\n",
      "step 731: 1.567717432975769\n",
      "step 732: 1.5114820003509521\n",
      "step 733: 1.5187866687774658\n",
      "step 734: 1.5425491333007812\n",
      "step 735: 1.5358864068984985\n",
      "step 736: 1.5248074531555176\n",
      "step 737: 1.5419601202011108\n",
      "step 738: 1.5200486183166504\n",
      "step 739: 1.5250321626663208\n",
      "step 740: 1.5470882654190063\n",
      "step 741: 1.522713303565979\n",
      "step 742: 1.556182861328125\n",
      "step 743: 1.5089787244796753\n",
      "step 744: 1.576200008392334\n",
      "step 745: 1.520459532737732\n",
      "step 746: 1.5188920497894287\n",
      "step 747: 1.5450141429901123\n",
      "step 748: 1.5020790100097656\n",
      "step 749: 1.5390013456344604\n",
      "step 750: 1.5588494539260864\n",
      "step 751: 1.5304290056228638\n",
      "step 752: 1.514639973640442\n",
      "step 753: 1.500614047050476\n",
      "step 754: 1.5535510778427124\n",
      "step 755: 1.4750423431396484\n",
      "step 756: 1.5212781429290771\n",
      "step 757: 1.5450165271759033\n",
      "step 758: 1.583048939704895\n",
      "step 759: 1.5370765924453735\n",
      "step 760: 1.52347993850708\n",
      "step 761: 1.5415087938308716\n",
      "step 762: 1.549357295036316\n",
      "step 763: 1.5210498571395874\n",
      "step 764: 1.5012277364730835\n",
      "step 765: 1.4985288381576538\n",
      "step 766: 1.5573655366897583\n",
      "step 767: 1.504655122756958\n",
      "step 768: 1.552106499671936\n",
      "step 769: 1.5290740728378296\n",
      "step 770: 1.5082972049713135\n",
      "step 771: 1.5300363302230835\n",
      "step 772: 1.5174192190170288\n",
      "step 773: 1.5469756126403809\n",
      "step 774: 1.5273417234420776\n",
      "step 775: 1.5683467388153076\n",
      "step 776: 1.5199663639068604\n",
      "step 777: 1.5934358835220337\n",
      "step 778: 1.5639034509658813\n",
      "step 779: 1.5077481269836426\n",
      "step 780: 1.5215660333633423\n",
      "step 781: 1.5248267650604248\n",
      "step 782: 1.5711283683776855\n",
      "step 783: 1.5282254219055176\n",
      "step 784: 1.4932928085327148\n",
      "step 785: 1.5746937990188599\n",
      "step 786: 1.5571633577346802\n",
      "step 787: 1.5259257555007935\n",
      "step 788: 1.5592554807662964\n",
      "step 789: 1.5412697792053223\n",
      "step 790: 1.5215401649475098\n",
      "step 791: 1.5094221830368042\n",
      "step 792: 1.5437772274017334\n",
      "step 793: 1.5276039838790894\n",
      "step 794: 1.508839726448059\n",
      "step 795: 1.5586674213409424\n",
      "step 796: 1.5308483839035034\n",
      "step 797: 1.5377377271652222\n",
      "step 798: 1.5337907075881958\n",
      "step 799: 1.4853153228759766\n",
      "step 800: 1.5582472085952759\n",
      "step 801: 1.5515626668930054\n",
      "step 802: 1.5400843620300293\n",
      "step 803: 1.5287652015686035\n",
      "step 804: 1.5093433856964111\n",
      "step 805: 1.5087591409683228\n",
      "step 806: 1.5288077592849731\n",
      "step 807: 1.5364105701446533\n",
      "step 808: 1.5232030153274536\n",
      "step 809: 1.4873520135879517\n",
      "step 810: 1.5102336406707764\n",
      "step 811: 1.536292552947998\n",
      "step 812: 1.5472941398620605\n",
      "step 813: 1.578866958618164\n",
      "step 814: 1.541700839996338\n",
      "step 815: 1.569779872894287\n",
      "step 816: 1.544677734375\n",
      "step 817: 1.503222107887268\n",
      "step 818: 1.5327527523040771\n",
      "step 819: 1.5257796049118042\n",
      "step 820: 1.5094410181045532\n",
      "step 821: 1.5280953645706177\n",
      "step 822: 1.5731154680252075\n",
      "step 823: 1.5638340711593628\n",
      "step 824: 1.501406192779541\n",
      "step 825: 1.5173283815383911\n",
      "step 826: 1.5356779098510742\n",
      "step 827: 1.4901189804077148\n",
      "step 828: 1.5357615947723389\n",
      "step 829: 1.5364222526550293\n",
      "step 830: 1.5602202415466309\n",
      "step 831: 1.5273672342300415\n",
      "step 832: 1.5268335342407227\n",
      "step 833: 1.5344301462173462\n",
      "step 834: 1.543172836303711\n",
      "step 835: 1.5210942029953003\n",
      "step 836: 1.5113927125930786\n",
      "step 837: 1.5520399808883667\n",
      "step 838: 1.4823888540267944\n",
      "step 839: 1.5127336978912354\n",
      "step 840: 1.5437695980072021\n",
      "step 841: 1.5002872943878174\n",
      "step 842: 1.532976508140564\n",
      "step 843: 1.511844515800476\n",
      "step 844: 1.5609242916107178\n",
      "step 845: 1.5743134021759033\n",
      "step 846: 1.5467517375946045\n",
      "step 847: 1.5491551160812378\n",
      "step 848: 1.5273523330688477\n",
      "step 849: 1.5144085884094238\n",
      "step 850: 1.5000295639038086\n",
      "step 851: 1.5180994272232056\n",
      "step 852: 1.5795892477035522\n",
      "step 853: 1.4867188930511475\n",
      "step 854: 1.532231092453003\n",
      "step 855: 1.5418708324432373\n",
      "step 856: 1.5304831266403198\n",
      "step 857: 1.5183703899383545\n",
      "step 858: 1.5312447547912598\n",
      "step 859: 1.5325976610183716\n",
      "step 860: 1.5299242734909058\n",
      "step 861: 1.5251562595367432\n",
      "step 862: 1.546134352684021\n",
      "step 863: 1.571975827217102\n",
      "step 864: 1.5491971969604492\n",
      "step 865: 1.5338759422302246\n",
      "step 866: 1.493468999862671\n",
      "step 867: 1.5077991485595703\n",
      "step 868: 1.5251455307006836\n",
      "step 869: 1.577997088432312\n",
      "step 870: 1.5095951557159424\n",
      "step 871: 1.5469145774841309\n",
      "step 872: 1.5338352918624878\n",
      "step 873: 1.5485777854919434\n",
      "step 874: 1.508901834487915\n",
      "step 875: 1.5372943878173828\n",
      "step 876: 1.4991523027420044\n",
      "step 877: 1.5440618991851807\n",
      "step 878: 1.536302089691162\n",
      "step 879: 1.5098559856414795\n",
      "step 880: 1.5224441289901733\n",
      "step 881: 1.493175983428955\n",
      "step 882: 1.5566307306289673\n",
      "step 883: 1.5423941612243652\n",
      "step 884: 1.5723761320114136\n",
      "step 885: 1.5386227369308472\n",
      "step 886: 1.5455083847045898\n",
      "step 887: 1.5349595546722412\n",
      "step 888: 1.5208736658096313\n",
      "step 889: 1.5085200071334839\n",
      "step 890: 1.5909383296966553\n",
      "step 891: 1.5118547677993774\n",
      "step 892: 1.558571457862854\n",
      "step 893: 1.520687222480774\n",
      "step 894: 1.5293383598327637\n",
      "step 895: 1.5243303775787354\n",
      "step 896: 1.544226050376892\n",
      "step 897: 1.5457589626312256\n",
      "step 898: 1.554002046585083\n",
      "step 899: 1.5686675310134888\n",
      "step 900: 1.5231359004974365\n",
      "step 901: 1.5142154693603516\n",
      "step 902: 1.5406378507614136\n",
      "step 903: 1.5854705572128296\n",
      "step 904: 1.5211572647094727\n",
      "step 905: 1.5271544456481934\n",
      "step 906: 1.4922685623168945\n",
      "step 907: 1.51410710811615\n",
      "step 908: 1.5581172704696655\n",
      "step 909: 1.5335792303085327\n",
      "step 910: 1.5426385402679443\n",
      "step 911: 1.4832686185836792\n",
      "step 912: 1.4976310729980469\n",
      "step 913: 1.5308645963668823\n",
      "step 914: 1.4771162271499634\n",
      "step 915: 1.5087913274765015\n",
      "step 916: 1.5757981538772583\n",
      "step 917: 1.5140122175216675\n",
      "step 918: 1.5807435512542725\n",
      "step 919: 1.5572595596313477\n",
      "step 920: 1.5501000881195068\n",
      "step 921: 1.5543334484100342\n",
      "step 922: 1.5587375164031982\n",
      "step 923: 1.5060162544250488\n",
      "step 924: 1.5118741989135742\n",
      "step 925: 1.5128145217895508\n",
      "step 926: 1.523197889328003\n",
      "step 927: 1.5349339246749878\n",
      "step 928: 1.5180892944335938\n",
      "step 929: 1.5466800928115845\n",
      "step 930: 1.5253791809082031\n",
      "step 931: 1.5195887088775635\n",
      "step 932: 1.5305238962173462\n",
      "step 933: 1.5228538513183594\n",
      "step 934: 1.522821307182312\n",
      "step 935: 1.5156019926071167\n",
      "step 936: 1.5142515897750854\n",
      "step 937: 1.5232946872711182\n",
      "step 938: 1.5097852945327759\n",
      "step 939: 1.5582772493362427\n",
      "step 940: 1.5691237449645996\n",
      "step 941: 1.5360157489776611\n",
      "step 942: 1.5057337284088135\n",
      "step 943: 1.5437227487564087\n",
      "step 944: 1.5244196653366089\n",
      "step 945: 1.5491788387298584\n",
      "step 946: 1.5195480585098267\n",
      "step 947: 1.540393352508545\n",
      "step 948: 1.5541353225708008\n",
      "step 949: 1.5093328952789307\n",
      "step 950: 1.4920321702957153\n",
      "step 951: 1.5118404626846313\n",
      "step 952: 1.5485767126083374\n",
      "step 953: 1.54991614818573\n",
      "step 954: 1.4921168088912964\n",
      "step 955: 1.4881309270858765\n",
      "step 956: 1.5030254125595093\n",
      "step 957: 1.5040862560272217\n",
      "step 958: 1.4920462369918823\n",
      "step 959: 1.5224941968917847\n",
      "step 960: 1.5198591947555542\n",
      "step 961: 1.5553174018859863\n",
      "step 962: 1.5533866882324219\n",
      "step 963: 1.5217206478118896\n",
      "step 964: 1.531138300895691\n",
      "step 965: 1.580712914466858\n",
      "step 966: 1.471177101135254\n",
      "step 967: 1.527674674987793\n",
      "step 968: 1.5565509796142578\n",
      "step 969: 1.5189783573150635\n",
      "step 970: 1.5335243940353394\n",
      "step 971: 1.4995149374008179\n",
      "step 972: 1.5060847997665405\n",
      "step 973: 1.489278793334961\n",
      "step 974: 1.516905665397644\n",
      "step 975: 1.4930319786071777\n",
      "step 976: 1.5408523082733154\n",
      "step 977: 1.5180312395095825\n",
      "step 978: 1.533216118812561\n",
      "step 979: 1.5160479545593262\n",
      "step 980: 1.5229663848876953\n",
      "step 981: 1.525749921798706\n",
      "step 982: 1.5169051885604858\n",
      "step 983: 1.530018925666809\n",
      "step 984: 1.5672993659973145\n",
      "step 985: 1.5217466354370117\n",
      "step 986: 1.5005884170532227\n",
      "step 987: 1.5515881776809692\n",
      "step 988: 1.5178658962249756\n",
      "step 989: 1.4810794591903687\n",
      "step 990: 1.5260045528411865\n",
      "step 991: 1.5248557329177856\n",
      "step 992: 1.5174620151519775\n",
      "step 993: 1.5514804124832153\n",
      "step 994: 1.5367710590362549\n",
      "step 995: 1.5440040826797485\n",
      "step 996: 1.4988212585449219\n",
      "step 997: 1.5275206565856934\n",
      "step 998: 1.5185046195983887\n",
      "step 999: 1.5021191835403442\n",
      "step 1000: 1.543261170387268\n",
      "step 1001: 1.5357918739318848\n",
      "step 1002: 1.5589772462844849\n",
      "step 1003: 1.5518697500228882\n",
      "step 1004: 1.5141030550003052\n",
      "step 1005: 1.5513430833816528\n",
      "step 1006: 1.5164622068405151\n",
      "step 1007: 1.4767237901687622\n",
      "step 1008: 1.4707661867141724\n",
      "step 1009: 1.5057313442230225\n",
      "step 1010: 1.5512515306472778\n",
      "step 1011: 1.5517821311950684\n",
      "step 1012: 1.518723487854004\n",
      "step 1013: 1.586991310119629\n",
      "step 1014: 1.5152984857559204\n",
      "step 1015: 1.4833877086639404\n",
      "step 1016: 1.5236635208129883\n",
      "step 1017: 1.5111826658248901\n",
      "step 1018: 1.5083034038543701\n",
      "step 1019: 1.5782493352890015\n",
      "step 1020: 1.5059903860092163\n",
      "step 1021: 1.570780634880066\n",
      "step 1022: 1.5096250772476196\n",
      "step 1023: 1.5135236978530884\n",
      "step 1024: 1.5637787580490112\n",
      "step 1025: 1.5156078338623047\n",
      "step 1026: 1.5024802684783936\n",
      "step 1027: 1.5245985984802246\n",
      "step 1028: 1.5473110675811768\n",
      "step 1029: 1.519317865371704\n",
      "step 1030: 1.5593712329864502\n",
      "step 1031: 1.5402551889419556\n",
      "step 1032: 1.5552423000335693\n",
      "step 1033: 1.5268863439559937\n",
      "step 1034: 1.534479022026062\n",
      "step 1035: 1.5412931442260742\n",
      "step 1036: 1.550536870956421\n",
      "step 1037: 1.572853684425354\n",
      "step 1038: 1.500848412513733\n",
      "step 1039: 1.53598153591156\n",
      "step 1040: 1.5094820261001587\n",
      "step 1041: 1.5099661350250244\n",
      "step 1042: 1.5561846494674683\n",
      "step 1043: 1.5672607421875\n",
      "step 1044: 1.5085718631744385\n",
      "step 1045: 1.5636495351791382\n",
      "step 1046: 1.5485414266586304\n",
      "step 1047: 1.5466721057891846\n",
      "step 1048: 1.527255654335022\n",
      "step 1049: 1.5332387685775757\n",
      "step 1050: 1.5010817050933838\n",
      "step 1051: 1.5206729173660278\n",
      "step 1052: 1.521844506263733\n",
      "step 1053: 1.5271202325820923\n",
      "step 1054: 1.5339329242706299\n",
      "step 1055: 1.5113884210586548\n",
      "step 1056: 1.5292468070983887\n",
      "step 1057: 1.5105661153793335\n",
      "step 1058: 1.5343602895736694\n",
      "step 1059: 1.515809178352356\n",
      "step 1060: 1.5229780673980713\n",
      "step 1061: 1.5076878070831299\n",
      "step 1062: 1.5114021301269531\n",
      "step 1063: 1.5138896703720093\n",
      "step 1064: 1.5128533840179443\n",
      "step 1065: 1.504372239112854\n",
      "step 1066: 1.5163573026657104\n",
      "step 1067: 1.5425869226455688\n",
      "step 1068: 1.5097910165786743\n",
      "step 1069: 1.5076121091842651\n",
      "step 1070: 1.539875864982605\n",
      "step 1071: 1.5308128595352173\n",
      "step 1072: 1.5485036373138428\n",
      "step 1073: 1.5582560300827026\n",
      "step 1074: 1.5299807786941528\n",
      "step 1075: 1.553631067276001\n",
      "step 1076: 1.5173406600952148\n",
      "step 1077: 1.4981474876403809\n",
      "step 1078: 1.5120998620986938\n",
      "step 1079: 1.5263780355453491\n",
      "step 1080: 1.5138672590255737\n",
      "step 1081: 1.5350477695465088\n",
      "step 1082: 1.5038738250732422\n",
      "step 1083: 1.535789966583252\n",
      "step 1084: 1.5199739933013916\n",
      "step 1085: 1.4957743883132935\n",
      "step 1086: 1.5094921588897705\n",
      "step 1087: 1.5086456537246704\n",
      "step 1088: 1.5778322219848633\n",
      "step 1089: 1.5812474489212036\n",
      "step 1090: 1.4813746213912964\n",
      "step 1091: 1.5021668672561646\n",
      "step 1092: 1.5584338903427124\n",
      "step 1093: 1.545241355895996\n",
      "step 1094: 1.5201843976974487\n",
      "step 1095: 1.5202841758728027\n",
      "step 1096: 1.543481469154358\n",
      "step 1097: 1.5231064558029175\n",
      "step 1098: 1.5333033800125122\n",
      "step 1099: 1.553457260131836\n",
      "step 1100: 1.5080771446228027\n",
      "step 1101: 1.552704930305481\n",
      "step 1102: 1.5221760272979736\n",
      "step 1103: 1.486122727394104\n",
      "step 1104: 1.492311954498291\n",
      "step 1105: 1.5488417148590088\n",
      "step 1106: 1.514474630355835\n",
      "step 1107: 1.5177156925201416\n",
      "step 1108: 1.529237151145935\n",
      "step 1109: 1.4892734289169312\n",
      "step 1110: 1.5200870037078857\n",
      "step 1111: 1.5052977800369263\n",
      "step 1112: 1.4811758995056152\n",
      "step 1113: 1.5130321979522705\n",
      "step 1114: 1.5204344987869263\n",
      "step 1115: 1.5118212699890137\n",
      "step 1116: 1.5413693189620972\n",
      "step 1117: 1.467305064201355\n",
      "step 1118: 1.511793613433838\n",
      "step 1119: 1.519238829612732\n",
      "step 1120: 1.541772484779358\n",
      "step 1121: 1.4826304912567139\n",
      "step 1122: 1.5875951051712036\n",
      "step 1123: 1.5325502157211304\n",
      "step 1124: 1.5141346454620361\n",
      "step 1125: 1.5279479026794434\n",
      "step 1126: 1.4852683544158936\n",
      "step 1127: 1.4928301572799683\n",
      "step 1128: 1.4921826124191284\n",
      "step 1129: 1.4954332113265991\n",
      "step 1130: 1.5451644659042358\n",
      "step 1131: 1.5006715059280396\n",
      "step 1132: 1.511304259300232\n",
      "step 1133: 1.5133486986160278\n",
      "step 1134: 1.5159884691238403\n",
      "step 1135: 1.504089593887329\n",
      "step 1136: 1.4823246002197266\n",
      "step 1137: 1.5290595293045044\n",
      "step 1138: 1.5249278545379639\n",
      "step 1139: 1.5111782550811768\n",
      "step 1140: 1.5000901222229004\n",
      "step 1141: 1.510730266571045\n",
      "step 1142: 1.544553518295288\n",
      "step 1143: 1.5195763111114502\n",
      "step 1144: 1.5298625230789185\n",
      "step 1145: 1.5061315298080444\n",
      "step 1146: 1.5430341958999634\n",
      "step 1147: 1.5186930894851685\n",
      "step 1148: 1.540389060974121\n",
      "step 1149: 1.5338801145553589\n",
      "step 1150: 1.4829550981521606\n",
      "step 1151: 1.5308291912078857\n",
      "step 1152: 1.538619875907898\n",
      "step 1153: 1.5039712190628052\n",
      "step 1154: 1.504491925239563\n",
      "step 1155: 1.532291054725647\n",
      "step 1156: 1.4939651489257812\n",
      "step 1157: 1.4993159770965576\n",
      "step 1158: 1.5399742126464844\n",
      "step 1159: 1.5353024005889893\n",
      "step 1160: 1.5163460969924927\n",
      "step 1161: 1.5187022686004639\n",
      "step 1162: 1.528298258781433\n",
      "step 1163: 1.517363429069519\n",
      "step 1164: 1.5223782062530518\n",
      "step 1165: 1.5566304922103882\n",
      "step 1166: 1.5351297855377197\n",
      "step 1167: 1.573625922203064\n",
      "step 1168: 1.5636152029037476\n",
      "step 1169: 1.5041558742523193\n",
      "step 1170: 1.5267696380615234\n",
      "step 1171: 1.5028076171875\n",
      "step 1172: 1.5132662057876587\n",
      "step 1173: 1.5305542945861816\n",
      "step 1174: 1.5271668434143066\n",
      "step 1175: 1.5452793836593628\n",
      "step 1176: 1.5330430269241333\n",
      "step 1177: 1.508094310760498\n",
      "step 1178: 1.5471827983856201\n",
      "step 1179: 1.521280288696289\n",
      "step 1180: 1.5050603151321411\n",
      "step 1181: 1.5021381378173828\n",
      "step 1182: 1.5159146785736084\n",
      "step 1183: 1.4992817640304565\n",
      "step 1184: 1.54258131980896\n",
      "step 1185: 1.537708044052124\n",
      "step 1186: 1.5347568988800049\n",
      "step 1187: 1.5246460437774658\n",
      "step 1188: 1.516205072402954\n",
      "step 1189: 1.5591027736663818\n",
      "step 1190: 1.4933980703353882\n",
      "step 1191: 1.523970365524292\n",
      "step 1192: 1.4968218803405762\n",
      "step 1193: 1.5030980110168457\n",
      "step 1194: 1.5081630945205688\n",
      "step 1195: 1.5068577527999878\n",
      "step 1196: 1.5136384963989258\n",
      "step 1197: 1.4868618249893188\n",
      "step 1198: 1.5627528429031372\n",
      "step 1199: 1.5220727920532227\n",
      "step 1200: 1.5170822143554688\n",
      "step 1201: 1.5715574026107788\n",
      "step 1202: 1.5242724418640137\n",
      "step 1203: 1.5205397605895996\n",
      "step 1204: 1.5186259746551514\n",
      "step 1205: 1.4773637056350708\n",
      "step 1206: 1.5049651861190796\n",
      "step 1207: 1.4837476015090942\n",
      "step 1208: 1.5554710626602173\n",
      "step 1209: 1.5440462827682495\n",
      "step 1210: 1.4911576509475708\n",
      "step 1211: 1.5982030630111694\n",
      "step 1212: 1.5003479719161987\n",
      "step 1213: 1.503331184387207\n",
      "step 1214: 1.5685908794403076\n",
      "step 1215: 1.5120443105697632\n",
      "step 1216: 1.5560203790664673\n",
      "step 1217: 1.505424976348877\n",
      "step 1218: 1.5288193225860596\n",
      "step 1219: 1.4886655807495117\n",
      "step 1220: 1.490020990371704\n",
      "step 1221: 1.4918735027313232\n",
      "step 1222: 1.507236123085022\n",
      "step 1223: 1.5464823246002197\n",
      "step 1224: 1.5422285795211792\n",
      "step 1225: 1.5298713445663452\n",
      "step 1226: 1.5080845355987549\n",
      "step 1227: 1.54511296749115\n",
      "step 1228: 1.5185993909835815\n",
      "step 1229: 1.5185370445251465\n",
      "step 1230: 1.4918853044509888\n",
      "step 1231: 1.5354567766189575\n",
      "step 1232: 1.5560294389724731\n",
      "step 1233: 1.5236186981201172\n",
      "step 1234: 1.5464636087417603\n",
      "step 1235: 1.5002777576446533\n",
      "step 1236: 1.4799803495407104\n",
      "step 1237: 1.512988567352295\n",
      "step 1238: 1.4730710983276367\n",
      "step 1239: 1.5163229703903198\n",
      "step 1240: 1.5054727792739868\n",
      "step 1241: 1.4904584884643555\n",
      "step 1242: 1.501017689704895\n",
      "step 1243: 1.488525152206421\n",
      "step 1244: 1.4867095947265625\n",
      "step 1245: 1.5168431997299194\n",
      "step 1246: 1.5143707990646362\n",
      "step 1247: 1.5130369663238525\n",
      "step 1248: 1.557155728340149\n",
      "step 1249: 1.5114738941192627\n",
      "step 1250: 1.5166959762573242\n",
      "step 1251: 1.5000064373016357\n",
      "step 1252: 1.4938229322433472\n",
      "step 1253: 1.5174161195755005\n",
      "step 1254: 1.5268880128860474\n",
      "step 1255: 1.47880220413208\n",
      "step 1256: 1.4982727766036987\n",
      "step 1257: 1.531492829322815\n",
      "step 1258: 1.552742838859558\n",
      "step 1259: 1.50719153881073\n",
      "step 1260: 1.5533040761947632\n",
      "step 1261: 1.5215617418289185\n",
      "step 1262: 1.4899688959121704\n",
      "step 1263: 1.532379150390625\n",
      "step 1264: 1.5288182497024536\n",
      "step 1265: 1.506962776184082\n",
      "step 1266: 1.5165226459503174\n",
      "step 1267: 1.5384632349014282\n",
      "step 1268: 1.5566661357879639\n",
      "step 1269: 1.5105400085449219\n",
      "step 1270: 1.5076936483383179\n",
      "step 1271: 1.515784740447998\n",
      "step 1272: 1.5782756805419922\n",
      "step 1273: 1.5075786113739014\n",
      "step 1274: 1.5191892385482788\n",
      "step 1275: 1.5163642168045044\n",
      "step 1276: 1.5033468008041382\n",
      "step 1277: 1.4954391717910767\n",
      "step 1278: 1.5269207954406738\n",
      "step 1279: 1.5037949085235596\n",
      "step 1280: 1.5079432725906372\n",
      "step 1281: 1.5091755390167236\n",
      "step 1282: 1.4888125658035278\n",
      "step 1283: 1.484385371208191\n",
      "step 1284: 1.5558369159698486\n",
      "step 1285: 1.5523792505264282\n",
      "step 1286: 1.5234864950180054\n",
      "step 1287: 1.485770344734192\n",
      "step 1288: 1.572439193725586\n",
      "step 1289: 1.5596733093261719\n",
      "step 1290: 1.497100591659546\n",
      "step 1291: 1.4966909885406494\n",
      "step 1292: 1.5013309717178345\n",
      "step 1293: 1.4691499471664429\n",
      "step 1294: 1.5158106088638306\n",
      "step 1295: 1.5011907815933228\n",
      "step 1296: 1.5307759046554565\n",
      "step 1297: 1.525286078453064\n",
      "step 1298: 1.5017404556274414\n",
      "step 1299: 1.5131090879440308\n",
      "step 1300: 1.4955462217330933\n",
      "step 1301: 1.4822944402694702\n",
      "step 1302: 1.5246849060058594\n",
      "step 1303: 1.4894347190856934\n",
      "step 1304: 1.501939296722412\n",
      "step 1305: 1.4867141246795654\n",
      "step 1306: 1.4830282926559448\n",
      "step 1307: 1.496900200843811\n",
      "step 1308: 1.5012708902359009\n",
      "step 1309: 1.494808316230774\n",
      "step 1310: 1.5025227069854736\n",
      "step 1311: 1.534394383430481\n",
      "step 1312: 1.5241700410842896\n",
      "step 1313: 1.4982513189315796\n",
      "step 1314: 1.5655174255371094\n",
      "step 1315: 1.532492995262146\n",
      "step 1316: 1.5006465911865234\n",
      "step 1317: 1.5239181518554688\n",
      "step 1318: 1.5344929695129395\n",
      "step 1319: 1.5769238471984863\n",
      "step 1320: 1.5113502740859985\n",
      "step 1321: 1.5157140493392944\n",
      "step 1322: 1.495904564857483\n",
      "step 1323: 1.484503149986267\n",
      "step 1324: 1.5260429382324219\n",
      "step 1325: 1.4975333213806152\n",
      "step 1326: 1.5501943826675415\n",
      "step 1327: 1.5063608884811401\n",
      "step 1328: 1.5820527076721191\n",
      "step 1329: 1.4989535808563232\n",
      "step 1330: 1.5070059299468994\n",
      "step 1331: 1.5129019021987915\n",
      "step 1332: 1.5248186588287354\n",
      "step 1333: 1.5241106748580933\n",
      "step 1334: 1.5213080644607544\n",
      "step 1335: 1.5592707395553589\n",
      "step 1336: 1.492061734199524\n",
      "step 1337: 1.5019187927246094\n",
      "step 1338: 1.5094108581542969\n",
      "step 1339: 1.4949312210083008\n",
      "step 1340: 1.4741151332855225\n",
      "step 1341: 1.4990646839141846\n",
      "step 1342: 1.507568120956421\n",
      "step 1343: 1.509800672531128\n",
      "step 1344: 1.496044635772705\n",
      "step 1345: 1.5291938781738281\n",
      "step 1346: 1.495337963104248\n",
      "step 1347: 1.4979379177093506\n",
      "step 1348: 1.5142571926116943\n",
      "step 1349: 1.5340341329574585\n",
      "step 1350: 1.5248656272888184\n",
      "step 1351: 1.4688866138458252\n",
      "step 1352: 1.4962975978851318\n",
      "step 1353: 1.5665619373321533\n",
      "step 1354: 1.4898629188537598\n",
      "step 1355: 1.4939016103744507\n",
      "step 1356: 1.5091345310211182\n",
      "step 1357: 1.5280593633651733\n",
      "step 1358: 1.5285230875015259\n",
      "step 1359: 1.500864028930664\n",
      "step 1360: 1.499907374382019\n",
      "step 1361: 1.5042692422866821\n",
      "step 1362: 1.517966389656067\n",
      "step 1363: 1.5505902767181396\n",
      "step 1364: 1.481520652770996\n",
      "step 1365: 1.532326340675354\n",
      "step 1366: 1.5588929653167725\n",
      "step 1367: 1.5682810544967651\n",
      "step 1368: 1.5483454465866089\n",
      "step 1369: 1.5009428262710571\n",
      "step 1370: 1.5005545616149902\n",
      "step 1371: 1.5400011539459229\n",
      "step 1372: 1.5498573780059814\n",
      "step 1373: 1.5298302173614502\n",
      "step 1374: 1.5338025093078613\n",
      "step 1375: 1.5173931121826172\n",
      "step 1376: 1.501080870628357\n",
      "step 1377: 1.4892158508300781\n",
      "step 1378: 1.5008301734924316\n",
      "step 1379: 1.5384552478790283\n",
      "step 1380: 1.474880576133728\n",
      "step 1381: 1.4808306694030762\n",
      "step 1382: 1.5394309759140015\n",
      "step 1383: 1.5164799690246582\n",
      "step 1384: 1.5186166763305664\n",
      "step 1385: 1.5381158590316772\n",
      "step 1386: 1.5418081283569336\n",
      "step 1387: 1.5140544176101685\n",
      "step 1388: 1.4733816385269165\n",
      "step 1389: 1.5235118865966797\n",
      "step 1390: 1.534423828125\n",
      "step 1391: 1.478712558746338\n",
      "step 1392: 1.5116181373596191\n",
      "step 1393: 1.5856531858444214\n",
      "step 1394: 1.53164803981781\n",
      "step 1395: 1.525957703590393\n",
      "step 1396: 1.5152703523635864\n",
      "step 1397: 1.4971389770507812\n",
      "step 1398: 1.5200417041778564\n",
      "step 1399: 1.5491613149642944\n",
      "step 1400: 1.5088804960250854\n",
      "step 1401: 1.5001168251037598\n",
      "step 1402: 1.5932998657226562\n",
      "step 1403: 1.5405396223068237\n",
      "step 1404: 1.5253573656082153\n",
      "step 1405: 1.4875787496566772\n",
      "step 1406: 1.5298539400100708\n",
      "step 1407: 1.5031306743621826\n",
      "step 1408: 1.525313138961792\n",
      "step 1409: 1.501847267150879\n",
      "step 1410: 1.5004812479019165\n",
      "step 1411: 1.5133157968521118\n",
      "step 1412: 1.5765200853347778\n",
      "step 1413: 1.5070112943649292\n",
      "step 1414: 1.5133588314056396\n",
      "step 1415: 1.513139247894287\n",
      "step 1416: 1.4853954315185547\n",
      "step 1417: 1.509826421737671\n",
      "step 1418: 1.5344130992889404\n",
      "step 1419: 1.5105230808258057\n",
      "step 1420: 1.4831078052520752\n",
      "step 1421: 1.5055979490280151\n",
      "step 1422: 1.5301411151885986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x0000025E92BF9BD0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\users\\gabri\\appdata\\local\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"C:\\users\\gabri\\appdata\\local\\miniconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1430, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"C:\\users\\gabri\\appdata\\local\\miniconda3\\lib\\multiprocessing\\process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"C:\\users\\gabri\\appdata\\local\\miniconda3\\lib\\multiprocessing\\popen_spawn_win32.py\", line 108, in wait\n",
      "    res = _winapi.WaitForSingleObject(int(self._handle), msecs)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, epochs):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Iterate over all data\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X,labels \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;66;03m# Send the data through the model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m         y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;66;03m# Get the loss\u001b[39;00m\n\u001b[0;32m     11\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_funct(y_hat, labels)\n",
      "File \u001b[1;32mC:\\users\\gabri\\appdata\\local\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[52], line 23\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\users\\gabri\\appdata\\local\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\users\\gabri\\appdata\\local\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mC:\\users\\gabri\\appdata\\local\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[15], line 61\u001b[0m, in \u001b[0;36mSparse_Conv.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     58\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Multiply the patches with the weights in order to calculate the conv (batch_size, outCh, HW)\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Add the biases\u001b[39;00m\n\u001b[0;32m     64\u001b[0m X \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 10\n",
    "steps = 0\n",
    "for epoch in range(0, epochs):\n",
    "    # Iterate over all data\n",
    "    for X,labels in data_loader:\n",
    "        # Send the data through the model\n",
    "        y_hat = model(X)\n",
    "        \n",
    "        # Get the loss\n",
    "        loss = loss_funct(y_hat, labels)\n",
    "        \n",
    "        # Backprop the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update model\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        steps += 1\n",
    "        print(f\"step {steps}: {loss.detach().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e27775-8cb5-4715-b692-0918c60f7e69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
