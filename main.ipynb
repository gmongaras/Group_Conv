{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3f6714b-4011-4d4c-a8c2-7c1b2d12463d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb1cf2f-27a0-43fe-a93e-4d0e7c4dca44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac91ddc-b2b2-4fad-9678-a2e4358bd963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b0bdf86-7669-489e-81e6-0f16c29ad338",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's say I have an image with C channels of size LW\n",
    "C = 10\n",
    "L = W = 15\n",
    "image = torch.rand(C, L, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07b9c022-5f36-46f2-90c9-456c4436f803",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 3, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's say I want to do a convolution over the image\n",
    "# with a single output channel and a kernel of size 3.\n",
    "# The weight will be of shape (1, C, 3, 3). This is a single\n",
    "# kernel which performs a convolution over all of C\n",
    "conv = nn.Conv2d(C, 1, 3)\n",
    "conv.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eaffbe91-d178-4299-a880-f5cbf564c942",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 13])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The output will be of shape (1, L-2, W-2)\n",
    "conv(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50a733a7-3af3-4cf2-a918-24e5c46ed3c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 3, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What if I now have a kernel that takes in C/2 and we split the\n",
    "# inputs up into a batch size of 2 by slicing the number of channels\n",
    "# into 2? So two images of shape (C/2, L, W)\n",
    "conv2 = nn.Conv2d(C//2, 1, 3)\n",
    "conv2.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c2d8c78-a6c5-46e2-985b-dc85cb5df9f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 13, 13])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The weight has half the size now reducing the parameters by 1/2. Now,\n",
    "# the output will be of shape (2, L-2, W-2) where the convolution\n",
    "# convolvs the first half and second half\n",
    "image2 = torch.stack((image[:5], image[5:]), dim=0)\n",
    "conv2(image2).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd5de0-35c9-44fb-a1a4-08f8e5f8dd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32938e58-30ca-4bea-aabf-e09a2718f2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1cf66a-9b2e-4131-b220-7e9e9cee1ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382d943a-5540-4c36-878c-57c62d361ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "253b30ed-e0d2-48aa-9642-0eac6e7c9540",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What if we make a convoltuion that only looks at a subset of the filters?\n",
    "# For now, the conv size if just 1\n",
    "class Sub_Conv(nn.Module):\n",
    "    def __init__(self, inCh, kernel, sub):\n",
    "        super(Sub_Conv, self).__init__()\n",
    "        \n",
    "        assert sub <= inCh\n",
    "        self.sub = sub\n",
    "        \n",
    "        self.conv = nn.Conv2d(sub, 1, kernel)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # Take the subset\n",
    "        if len(X.shape) == 4:\n",
    "            X = X[:, :self.sub]\n",
    "        elif len(X.shape) == 3:\n",
    "            X = X[:self.sub]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return self.conv(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e531caa3-3fe0-4e57-89ef-6f180824988e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_conv = Sub_Conv(C, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60cdd7ea-32cf-4bfb-8c45-205e2bdf7015",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 15, 15])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa791b1f-795c-4f2f-8077-b1e77f4496d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 13])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_conv(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2320f24e-bddb-410e-9474-f36091482307",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now let's try this with multiple output channels and a randomized\n",
    "# subset of inputs channels\n",
    "class Rand_Conv(nn.Module):\n",
    "    def __init__(self, inCh, outCh, kernel, sub):\n",
    "        super(Rand_Conv, self).__init__()\n",
    "        \n",
    "        assert sub <= inCh\n",
    "        self.sub = sub\n",
    "        \n",
    "        self.convs = nn.ParameterList([nn.Conv2d(sub, 1, kernel) for i in range(0, outCh)])\n",
    "        \n",
    "    def forward(self, X):\n",
    "        if len(X.shape) == 3:\n",
    "            X = X.unsqueeze(0)\n",
    "        \n",
    "        # Output tensor\n",
    "        out = []\n",
    "        \n",
    "        # Iterate over all convolutions\n",
    "        for c in self.convs:\n",
    "            # Get a random subset\n",
    "            idx = torch.randperm(X.shape[1])\n",
    "            \n",
    "            # Convolution\n",
    "            out.append(c(X[:, idx][:, :self.sub]).squeeze())\n",
    "        \n",
    "        # Stack the output and return it\n",
    "        return torch.stack(out).permute(1, 0, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3a7ccb4d-ae12-4dac-a534-9d91afd803a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_conv2 = Sub_Conv2(10, 4, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "32a92800-1b4b-4167-a04e-08d706f39954",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 15, 15])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5d2cdf4f-5c8f-4bf7-b3e2-2be595737d8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 13, 13])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_conv2(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b328de83-3ba5-4637-98ff-9d7415d0fa07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
